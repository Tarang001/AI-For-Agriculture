{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":126119,"databundleVersionId":14953781}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install numpy pandas matplotlib torch torchvision pillow rasterio scikit-learn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T06:32:39.122994Z","iopub.execute_input":"2026-02-25T06:32:39.123285Z","iopub.status.idle":"2026-02-25T06:32:42.666316Z","shell.execute_reply.started":"2026-02-25T06:32:39.123260Z","shell.execute_reply":"2026-02-25T06:32:42.665538Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.3.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\nRequirement already satisfied: rasterio in /usr/local/lib/python3.12/dist-packages (1.5.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\nRequirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\nRequirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio) (2.4.0)\nRequirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio) (25.4.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio) (2026.1.4)\nRequirement already satisfied: click!=8.2.*,>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio) (8.3.1)\nRequirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio) (0.7.2)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\n=============================================================================\nICPR 2026 — Multimodal Wheat Disease Classification  ─  v6 FINAL\nTarget: > 0.857 on competition validation set\n=============================================================================\n\nWHY v5 GOT 0.53:\n─────────────────\n  Band means collapse spatial patches to single numbers.\n  Health vs Rust have nearly IDENTICAL mean spectra.\n  SVM/RF on means = predicting nearly random for that pair.\n\nWHAT ACTUALLY DISCRIMINATES Health vs Rust:\n────────────────────────────────────────────\n  RUST creates:\n    • High LOCAL VARIANCE in red-edge bands (patchy lesions)\n    • High ENTROPY in NIR texture (non-uniform canopy)\n    • SKEWED pixel distribution (some pixels very bright = lesion spots)\n    • Low chlorophyll in RED band but HETEROGENEOUS (not uniform)\n\n  HEALTHY creates:\n    • Low variance (uniform green canopy)\n    • Symmetric pixel distributions\n    • Smooth spatial gradients\n\nSOLUTION — Three-part approach:\n─────────────────────────────────\n  PART A: Rich spatial texture features (the key insight)\n    - Per-band: mean, std, skew, kurtosis, p10,p25,p50,p75,p90\n    - GLCM-like: local variance map stats, co-occurrence energy\n    - Within-patch NDVI map: std, skew, fraction of stressed pixels\n    - Red-edge gradient magnitude statistics\n    - HS: above for ALL 101 bands → captures spectral heterogeneity\n\n  PART B: CNN on actual image patches (spatial learning)\n    - EfficientNet-B0 fine-tuned on RGB (224×224) — full fine-tuning\n    - Custom CNN on HS patch (101 bands, spatial) — learns texture\n    - Cross-validated, trained with MixUp + CutMix + strong augmentation\n\n  PART C: Stacking ensemble\n    - Train SVM/XGBoost/RF on PART A features\n    - Combine with PART B CNN softmax outputs via logistic regression meta-learner\n    - Final prediction = weighted average\n\nThis approach is grounded in what remote sensing literature actually uses\nfor disease detection with small datasets.\n=============================================================================\n\"\"\"\n\nimport os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n\nimport time, warnings, random\nfrom pathlib import Path\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom scipy import ndimage\nfrom scipy.stats import skew, kurtosis\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport rasterio\n\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\nimport xgboost as xgb\n\nwarnings.filterwarnings(\"ignore\")\n\n# =============================================================================\n# 0.  CONFIG\n# =============================================================================\n\nCOMP_ROOT  = \"/kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026\"\nDATA_ROOT  = os.path.join(COMP_ROOT, \"Kaggle_Prepared\")\nTRAIN_DIR  = os.path.join(DATA_ROOT, \"train\")\nVAL_DIR    = os.path.join(DATA_ROOT, \"val\")\nOUTPUT_DIR = \"/kaggle/working\"\nSUBMIT_CSV = os.path.join(OUTPUT_DIR, \"submission.csv\")\n\nCLASSES    = [\"Health\", \"Rust\", \"Other\"]\nN_CLS      = 3\nPREFIX_MAP = {\"health\": 0, \"rust\": 1, \"other\": 2}\n\nSEED       = 42\nN_FOLDS    = 5\nHS_START   = 10\nHS_END     = 111          # 101 valid HS bands\n\n# CNN settings\nIMG_SIZE   = 224\nBATCH_SIZE = 16\nN_EPOCHS   = 80           # with early stop, usually ~40-50 effective epochs\nLR_HEAD    = 1e-3         # new layers\nLR_BACK    = 5e-5         # pretrained backbone\nPATIENCE   = 15\nWARMUP     = 3\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# =============================================================================\n# 1.  SEED + DEVICE\n# =============================================================================\n\ndef set_seed(s):\n    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)\n    torch.backends.cudnn.deterministic = True\n\nset_seed(SEED)\n\ndef get_device():\n    if not torch.cuda.is_available():\n        print(\"[DEVICE] CPU only\"); return torch.device(\"cpu\")\n    try:\n        _ = torch.zeros(1).cuda() + 1\n        torch.cuda.empty_cache()\n        print(f\"[DEVICE] {torch.cuda.get_device_name(0)}\")\n        return torch.device(\"cuda\")\n    except Exception as e:\n        print(f\"[DEVICE] CUDA failed: {e} → CPU\")\n        return torch.device(\"cpu\")\n\nDEVICE = get_device()\n\n# =============================================================================\n# 2.  FILE HELPERS\n# =============================================================================\n\ndef label_from_fn(fn):\n    stem = Path(fn).stem.lower()\n    for pfx, idx in PREFIX_MAP.items():\n        if stem.startswith(pfx): return idx\n    for pfx, idx in PREFIX_MAP.items():\n        if pfx in stem: return idx\n    return -1\n\ndef find_file(d, stem, exts=(\".tif\",\".tiff\",\".png\",\".jpg\")):\n    if not d or not os.path.isdir(d): return None\n    for ext in exts:\n        p = os.path.join(d, stem + ext)\n        if os.path.isfile(p): return p\n    return None\n\ndef load_tif(path):\n    \"\"\"GeoTIFF → float32 (C,H,W) per-band min-max normalised [0,1].\"\"\"\n    with rasterio.open(path) as src:\n        data = src.read().astype(np.float32)\n    for b in range(data.shape[0]):\n        lo, hi = data[b].min(), data[b].max()\n        data[b] = (data[b] - lo) / (hi - lo + 1e-8)\n    return data\n\n# =============================================================================\n# 3.  FEATURE EXTRACTION  ← the key fix vs v5\n#     We extract SPATIAL TEXTURE features, not just means\n# =============================================================================\n\ndef band_stats(band_2d: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Extract 12 statistics from a single 2D band.\n    These capture DISTRIBUTION SHAPE, not just central tendency.\n    Rust lesions create high variance + positive skew in affected bands.\n    \"\"\"\n    flat = band_2d.flatten().astype(np.float64)\n    if flat.std() < 1e-8:   # constant band — return zeros\n        return np.zeros(12, dtype=np.float32)\n    return np.array([\n        flat.mean(),\n        flat.std(),\n        float(skew(flat)),          # asymmetry: rust → positive skew\n        float(kurtosis(flat)),      # peakedness: healthy → low kurtosis\n        np.percentile(flat, 5),\n        np.percentile(flat, 10),\n        np.percentile(flat, 25),\n        np.percentile(flat, 50),\n        np.percentile(flat, 75),\n        np.percentile(flat, 90),\n        np.percentile(flat, 95),\n        # Local variance (texture) — variance of a 3×3 local mean subtracted\n        float(ndimage.uniform_filter(band_2d, size=3).flatten().std()),\n    ], dtype=np.float32)\n\n\ndef extract_ms_features(ms: np.ndarray) -> np.ndarray:\n    \"\"\"\n    5-band MS patch (5,H,W) → rich feature vector.\n\n    Structure:\n      5 bands × 12 stats           = 60\n      Per-pixel index maps (stats) = 60\n      Total                        = 120\n    \"\"\"\n    feats = []\n    b, g, r, re, nir = [ms[i] for i in range(5)]\n    e = 1e-8\n\n    # Per-band texture stats\n    for band in [b, g, r, re, nir]:\n        feats.append(band_stats(band))  # 12 each → 60\n\n    # Per-pixel vegetation index MAPS → stats on the distribution\n    # (captures within-patch heterogeneity)\n    def idx_stats(map_2d):\n        return band_stats(map_2d)  # same 12 stats\n\n    ndvi_map   = (nir - r)  / (nir + r  + e)\n    rendvi_map = (nir - re) / (nir + re + e)\n    ndre_map   = (re  - r)  / (re  + r  + e)\n    gndvi_map  = (nir - g)  / (nir + g  + e)\n    evi_map    = 2.5*(nir-r)/ (nir + 6*r - 7.5*b + 1 + e)\n\n    feats.append(idx_stats(ndvi_map))    # 12\n    feats.append(idx_stats(rendvi_map))  # 12\n    feats.append(idx_stats(ndre_map))    # 12\n    feats.append(idx_stats(gndvi_map))   # 12\n    feats.append(idx_stats(evi_map))     # 12\n\n    # Total: 5×12 + 5×12 = 120\n    return np.concatenate(feats).astype(np.float32)\n\n\ndef extract_hs_features(hs: np.ndarray) -> np.ndarray:\n    \"\"\"\n    101-band HS patch (101,H,W) → feature vector.\n\n    Strategy: per-band stats for ALL 101 bands (captures full spectral curve\n    shape AND its spatial variability), plus key region summaries.\n\n    Per-band: mean, std, skew, p25, p50, p75  → 6 stats × 101 = 606\n    Key regions: 6 summary features\n    Total: 612\n    \"\"\"\n    feats = []\n\n    for b in range(hs.shape[0]):\n        flat = hs[b].flatten().astype(np.float64)\n        if flat.std() < 1e-8:\n            feats.append(np.zeros(6, dtype=np.float32))\n        else:\n            feats.append(np.array([\n                flat.mean(),\n                flat.std(),\n                float(skew(flat)),\n                np.percentile(flat, 25),\n                np.percentile(flat, 50),\n                np.percentile(flat, 75),\n            ], dtype=np.float32))\n\n    # Key spectral region summaries (from EDA)\n    # Green peak ~band 15-20\n    gp_std     = hs[13:20].std()\n    # Red-edge slope bands ~53-65 (700-750 nm)\n    re_mean    = hs[53:65].mean()\n    re_std     = hs[53:65].std()     # HIGH for rust (patchy re-absorption)\n    re_slope   = hs[60:65].mean() - hs[53:58].mean()\n    # NIR plateau ~73-84 (781-821 nm)\n    nir_mean   = hs[73:84].mean()\n    nir_std    = hs[73:84].std()\n\n    feats.append(np.array([gp_std, re_mean, re_std, re_slope,\n                            nir_mean, nir_std], dtype=np.float32))\n\n    return np.concatenate(feats).astype(np.float32)   # 612\n\n\ndef build_all_features(split=\"train\", verbose=True):\n    \"\"\"\n    Load all samples for a split, extract features.\n    Returns X_ms (N,120), X_hs (N,612), labels (N,), stems list, paths dict.\n    \"\"\"\n    rgb_dir = os.path.join(TRAIN_DIR if split==\"train\" else VAL_DIR, \"RGB\")\n    ms_dir  = os.path.join(TRAIN_DIR if split==\"train\" else VAL_DIR, \"MS\")\n    hs_dir  = os.path.join(TRAIN_DIR if split==\"train\" else VAL_DIR, \"HS\")\n\n    files = sorted(f for f in os.listdir(rgb_dir)\n                   if f.lower().endswith((\".png\",\".jpg\",\".jpeg\")))\n    N = len(files)\n    if verbose: print(f\"  [{split}] {N} files\")\n\n    X_ms   = np.zeros((N, 120), dtype=np.float32)\n    X_hs   = np.zeros((N, 612), dtype=np.float32)\n    labels = np.full(N, -1, dtype=np.int32)\n    stems  = []\n    paths  = {\"rgb\": [], \"ms\": [], \"hs\": []}\n\n    for i, fn in enumerate(files):\n        stem = Path(fn).stem; stems.append(stem)\n        if split == \"train\":\n            labels[i] = label_from_fn(fn)\n\n        rgb_p = os.path.join(rgb_dir, fn)\n        ms_p  = find_file(ms_dir, stem)\n        hs_p  = find_file(hs_dir, stem)\n        paths[\"rgb\"].append(rgb_p)\n        paths[\"ms\"] .append(ms_p)\n        paths[\"hs\"] .append(hs_p)\n\n        if ms_p and os.path.isfile(ms_p):\n            try:    X_ms[i] = extract_ms_features(load_tif(ms_p))\n            except Exception as ex:\n                if verbose: print(f\"  [WARN MS] {stem}: {ex}\")\n\n        if hs_p and os.path.isfile(hs_p):\n            try:    X_hs[i] = extract_hs_features(load_tif(hs_p)[HS_START:HS_END])\n            except Exception as ex:\n                if verbose: print(f\"  [WARN HS] {stem}: {ex}\")\n\n        if verbose and (i+1) % 200 == 0:\n            print(f\"    {i+1}/{N}\")\n\n    return X_ms, X_hs, labels, stems, paths\n\n# =============================================================================\n# 4.  CLASSICAL ML  (on texture features — much stronger than band means)\n# =============================================================================\n\ndef build_classifiers():\n    \"\"\"Returns dict of sklearn pipelines.\"\"\"\n    return {\n        \"svm_rbf\": Pipeline([\n            (\"sc\",  StandardScaler()),\n            (\"clf\", SVC(C=10,  gamma=\"scale\", kernel=\"rbf\",\n                        probability=True, random_state=SEED,\n                        class_weight=\"balanced\")),\n        ]),\n        \"svm_c100\": Pipeline([\n            (\"sc\",  StandardScaler()),\n            (\"clf\", SVC(C=100, gamma=\"scale\", kernel=\"rbf\",\n                        probability=True, random_state=SEED,\n                        class_weight=\"balanced\")),\n        ]),\n        \"rf\": Pipeline([\n            (\"sc\",  StandardScaler()),\n            (\"clf\", RandomForestClassifier(\n                n_estimators=600, max_depth=None, min_samples_leaf=1,\n                max_features=\"sqrt\", class_weight=\"balanced\",\n                random_state=SEED, n_jobs=-1)),\n        ]),\n        \"et\": Pipeline([\n            (\"sc\",  StandardScaler()),\n            (\"clf\", ExtraTreesClassifier(\n                n_estimators=600, max_depth=None, min_samples_leaf=1,\n                max_features=\"sqrt\", class_weight=\"balanced\",\n                random_state=SEED, n_jobs=-1)),\n        ]),\n        \"xgb\": Pipeline([\n            (\"sc\",  StandardScaler()),\n            (\"clf\", xgb.XGBClassifier(\n                n_estimators=500, max_depth=5, learning_rate=0.03,\n                subsample=0.8, colsample_bytree=0.8, min_child_weight=3,\n                eval_metric=\"mlogloss\", random_state=SEED,\n                n_jobs=-1, verbosity=0)),\n        ]),\n        \"gb\": Pipeline([\n            (\"sc\",  StandardScaler()),\n            (\"clf\", GradientBoostingClassifier(\n                n_estimators=300, max_depth=4, learning_rate=0.05,\n                subsample=0.8, random_state=SEED)),\n        ]),\n    }\n\n# =============================================================================\n# 5.  CNN  — Fine-tuned EfficientNet-B3 on RGB\n#     B3 is stronger than B0, still fits in GPU memory\n# =============================================================================\n\nclass RGBDataset(Dataset):\n    def __init__(self, rgb_paths, labels=None, augment=False):\n        self.paths   = rgb_paths\n        self.labels  = labels\n        self.augment = augment\n        self.tf = self._build_tf(augment)\n\n    @staticmethod\n    def _build_tf(aug):\n        if aug:\n            return transforms.Compose([\n                transforms.Resize((256, 256)),\n                transforms.RandomCrop(IMG_SIZE),\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomVerticalFlip(),\n                transforms.RandomRotation(45),\n                transforms.ColorJitter(brightness=0.4, contrast=0.4,\n                                       saturation=0.3, hue=0.1),\n                transforms.RandomGrayscale(p=0.05),\n                transforms.ToTensor(),\n                transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n                transforms.RandomErasing(p=0.25, scale=(0.02,0.2)),\n            ])\n        return transforms.Compose([\n            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n        ])\n\n    def __len__(self): return len(self.paths)\n\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        try:\n            img = Image.open(p).convert(\"RGB\") if p and os.path.isfile(p) \\\n                  else Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE))\n        except:\n            img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE))\n        x = self.tf(img)\n        if self.labels is not None:\n            lbl = int(self.labels[idx])\n            assert 0 <= lbl < N_CLS, f\"Bad label {lbl}\"\n            return x, lbl\n        return x\n\n\ndef build_cnn():\n    \"\"\"EfficientNet-B3 with custom classification head.\"\"\"\n    model = models.efficientnet_b3(\n        weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n    in_feat = model.classifier[1].in_features\n    model.classifier = nn.Sequential(\n        nn.Dropout(0.4),\n        nn.Linear(in_feat, 512),\n        nn.BatchNorm1d(512),\n        nn.GELU(),\n        nn.Dropout(0.3),\n        nn.Linear(512, N_CLS),\n    )\n    return model\n\n\ndef mixup_data(x, y, alpha=0.3):\n    \"\"\"MixUp augmentation — interpolates pairs of samples.\"\"\"\n    if alpha <= 0: return x, y, y, 1.0\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0), device=x.device)\n    return (lam*x + (1-lam)*x[idx], y, y[idx], lam)\n\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1-lam) * criterion(pred, y_b)\n\n\ndef get_warmup_cosine_scheduler(optimizer, warmup_ep, total_ep, min_lr=1e-6):\n    def lr_lambda(ep):\n        if ep < warmup_ep:\n            return float(ep+1) / float(warmup_ep)\n        prog = float(ep - warmup_ep) / float(max(1, total_ep - warmup_ep))\n        cosine = 0.5 * (1.0 + np.cos(np.pi * prog))\n        base_lr = optimizer.param_groups[0][\"initial_lr\"]\n        return max(min_lr / base_lr, cosine)\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n\ndef train_cnn_fold(tr_paths, tr_labels, vl_paths, vl_labels, device):\n    \"\"\"Train one fold of the CNN. Returns best model + val probs.\"\"\"\n    model = build_cnn().to(device)\n\n    # Separate LR: backbone vs head\n    backbone_params = list(model.features.parameters())\n    head_params     = list(model.classifier.parameters())\n    optimizer = torch.optim.AdamW([\n        {\"params\": backbone_params, \"lr\": LR_BACK,\n         \"initial_lr\": LR_BACK},\n        {\"params\": head_params,     \"lr\": LR_HEAD,\n         \"initial_lr\": LR_HEAD},\n    ], weight_decay=1e-4)\n\n    scheduler = get_warmup_cosine_scheduler(optimizer, WARMUP, N_EPOCHS)\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n    tr_ds = RGBDataset(tr_paths, tr_labels, augment=True)\n    vl_ds = RGBDataset(vl_paths, vl_labels, augment=False)\n\n    # Weighted sampler for balanced batches\n    cls_counts = np.bincount(tr_labels, minlength=N_CLS)\n    weights    = 1.0 / cls_counts[tr_labels]\n    sampler    = WeightedRandomSampler(weights, len(weights), replacement=True)\n\n    tr_ld = DataLoader(tr_ds, batch_size=BATCH_SIZE, sampler=sampler,\n                       num_workers=0, drop_last=True)\n    vl_ld = DataLoader(vl_ds, batch_size=BATCH_SIZE, shuffle=False,\n                       num_workers=0)\n\n    best_acc   = 0.0\n    best_state = None\n    no_improve = 0\n\n    for ep in range(1, N_EPOCHS + 1):\n        # ── Train ────────────────────────────────────────────────────────────\n        model.train()\n        for x, y in tr_ld:\n            x, y = x.to(device), y.to(device)\n            x, ya, yb, lam = mixup_data(x, y, alpha=0.4)\n            optimizer.zero_grad()\n            loss = mixup_criterion(criterion, model(x), ya, yb, lam)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        scheduler.step()\n\n        # ── Validate ─────────────────────────────────────────────────────────\n        model.eval()\n        correct = total = 0\n        with torch.no_grad():\n            for x, y in vl_ld:\n                x, y   = x.to(device), y.to(device)\n                preds  = model(x).argmax(1)\n                correct += (preds == y).sum().item()\n                total   += y.size(0)\n        acc = correct / total\n\n        if acc > best_acc:\n            best_acc   = acc\n            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n            no_improve = 0\n        else:\n            no_improve += 1\n\n        if ep % 10 == 0:\n            print(f\"      ep {ep:03d} val_acc={acc:.4f}  best={best_acc:.4f}\")\n\n        if no_improve >= PATIENCE:\n            print(f\"      Early stop at ep {ep}. Best val acc={best_acc:.4f}\")\n            break\n\n    model.load_state_dict(best_state)\n    return model, best_acc\n\n\n@torch.no_grad()\ndef get_probs(model, paths, device, augment=False, n_tta=1):\n    \"\"\"\n    Get softmax probabilities.\n    If n_tta > 1, run n_tta augmented forward passes and average.\n    \"\"\"\n    model.eval()\n    ds = RGBDataset(paths, labels=None, augment=augment)\n    ld = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    all_probs = []\n    for x in ld:\n        logits = model(x.to(device))\n        all_probs.append(torch.softmax(logits, 1).cpu().numpy())\n    return np.vstack(all_probs)\n\n\n@torch.no_grad()\ndef get_probs_tta(model, paths, device, n_tta=5):\n    \"\"\"Average probabilities over n_tta augmented views.\"\"\"\n    all_runs = []\n    # 1 clean pass\n    all_runs.append(get_probs(model, paths, device, augment=False))\n    # n_tta-1 augmented passes\n    for _ in range(n_tta - 1):\n        all_runs.append(get_probs(model, paths, device, augment=True))\n    return np.mean(all_runs, axis=0)\n\n# =============================================================================\n# 6.  STACKING META-LEARNER\n#     Takes OOF probs from classical + CNN, learns optimal combination\n# =============================================================================\n\ndef build_meta_learner():\n    return LogisticRegression(\n        C=1.0, max_iter=1000, random_state=SEED,\n        multi_class=\"multinomial\", solver=\"lbfgs\")\n\n# =============================================================================\n# 7.  FULL CV PIPELINE\n# =============================================================================\n\ndef run_cv(X_feat, y, rgb_paths_all, comp_rgb_paths, device):\n    \"\"\"\n    5-Fold Stratified CV:\n      Each fold trains:\n        - Classical models on texture features (X_feat)\n        - CNN on RGB patches\n\n      Collects:\n        - OOF probs for all models\n        - Test probs for final ensemble\n\n    Returns:\n      oof_classical  : (N, 3) averaged classical OOF probs\n      oof_cnn        : (N, 3) CNN OOF probs\n      test_classical : (N_test, 3) classical test probs (fold-averaged)\n      test_cnn       : (N_test, 3) CNN test probs (fold-averaged, with TTA)\n    \"\"\"\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    N_test = len(comp_rgb_paths)\n\n    oof_classical  = np.zeros((len(X_feat), 3), dtype=np.float32)\n    oof_cnn        = np.zeros((len(X_feat), 3), dtype=np.float32)\n    test_classical_folds = []\n    test_cnn_folds       = []\n\n    for fold, (tr_idx, vl_idx) in enumerate(skf.split(X_feat, y)):\n        print(f\"\\n{'─'*55}\")\n        print(f\"  FOLD {fold+1}/{N_FOLDS}\")\n        print(f\"{'─'*55}\")\n\n        X_tr, X_vl = X_feat[tr_idx], X_feat[vl_idx]\n        y_tr, y_vl = y[tr_idx],      y[vl_idx]\n        rgb_tr = [rgb_paths_all[i] for i in tr_idx]\n        rgb_vl = [rgb_paths_all[i] for i in vl_idx]\n\n        # ── Classical models ──────────────────────────────────────────────────\n        print(f\"\\n  [Classical ML]\")\n        classifiers = build_classifiers()\n        fold_cl_oof = np.zeros((len(vl_idx), 3), dtype=np.float32)\n\n        for name, pipe in classifiers.items():\n            p = clone(pipe)\n            p.fit(X_tr, y_tr)\n            vl_prob = p.predict_proba(X_vl)\n            vl_acc  = accuracy_score(y_vl, vl_prob.argmax(1))\n            print(f\"    {name:<12} val_acc={vl_acc:.4f}\")\n            fold_cl_oof += vl_prob\n\n        oof_classical[vl_idx] = fold_cl_oof / len(classifiers)\n        cl_oof_acc = accuracy_score(y_vl, oof_classical[vl_idx].argmax(1))\n        print(f\"    {'ENSEMBLE':<12} val_acc={cl_oof_acc:.4f}\")\n\n        # ── CNN ───────────────────────────────────────────────────────────────\n        print(f\"\\n  [CNN EfficientNet-B3]\")\n        cnn_model, cnn_acc = train_cnn_fold(\n            rgb_tr, y_tr, rgb_vl, y_vl, device)\n        print(f\"    CNN best val_acc={cnn_acc:.4f}\")\n\n        oof_cnn[vl_idx] = get_probs(cnn_model, rgb_vl, device)\n        cnn_test_probs  = get_probs_tta(cnn_model, comp_rgb_paths, device,\n                                        n_tta=5)\n        test_cnn_folds.append(cnn_test_probs)\n\n        # Cleanup GPU memory after each fold\n        del cnn_model\n        if DEVICE.type == \"cuda\": torch.cuda.empty_cache()\n\n    # Average CNN test probs across folds\n    test_cnn = np.mean(test_cnn_folds, axis=0)\n\n    return oof_classical, oof_cnn, test_cnn\n\n\ndef cv_classical_on_test(X_feat_train, y_train, X_feat_test):\n    \"\"\"\n    Retrain classical models on all train data, predict on test.\n    Separate function to ensure correct test features are used.\n    \"\"\"\n    print(\"\\n  [Classical — final retrain on all 600 samples]\")\n    classifiers = build_classifiers()\n    all_probs   = []\n    for name, pipe in classifiers.items():\n        p = clone(pipe)\n        p.fit(X_feat_train, y_train)\n        prob = p.predict_proba(X_feat_test)\n        acc  = accuracy_score(y_train, p.predict(X_feat_train))\n        print(f\"    {name:<12} train_acc={acc:.4f}\")\n        all_probs.append(prob)\n    return np.mean(all_probs, axis=0)\n\n# =============================================================================\n# 8.  MAIN\n# =============================================================================\n\ndef main():\n    t0 = time.time()\n    print(\"\\n\" + \"=\"*60)\n    print(\"  ICPR 2026 Wheat Disease  —  v6 SPATIAL TEXTURE + CNN\")\n    print(\"=\"*60)\n\n    # ── Step 1: Extract features ──────────────────────────────────────────────\n    print(\"\\n[STEP 1] Extracting spatial texture features from TRAIN …\")\n    X_ms_tr, X_hs_tr, y_tr, stems_tr, paths_tr = build_all_features(\"train\")\n    print(f\"  MS features: {X_ms_tr.shape}  HS features: {X_hs_tr.shape}\")\n    assert (y_tr >= 0).all() and (y_tr < N_CLS).all(), \\\n        f\"Bad train labels: {np.unique(y_tr)}\"\n    counts = dict(zip(*np.unique(y_tr, return_counts=True)))\n    print(f\"  Label distribution: {counts}\")\n\n    print(\"\\n[STEP 1b] Extracting features from competition VAL …\")\n    X_ms_vl, X_hs_vl, _, stems_vl, paths_vl = build_all_features(\"val\")\n\n    # ── Step 2: Concatenate MS + HS features ─────────────────────────────────\n    print(\"\\n[STEP 2] Concatenating MS+HS texture features …\")\n    X_train = np.concatenate([X_ms_tr, X_hs_tr], axis=1)   # (600, 732)\n    X_test  = np.concatenate([X_ms_vl, X_hs_vl], axis=1)   # (300, 732)\n    print(f\"  Train feature matrix: {X_train.shape}\")\n    print(f\"  Test  feature matrix: {X_test.shape}\")\n\n    # ── Step 3: CV training ───────────────────────────────────────────────────\n    print(f\"\\n[STEP 3] {N_FOLDS}-Fold Cross-Validation …\")\n    oof_classical, oof_cnn, test_cnn = run_cv(\n        X_train, y_tr,\n        rgb_paths_all  = paths_tr[\"rgb\"],\n        comp_rgb_paths = paths_vl[\"rgb\"],\n        device         = DEVICE,\n    )\n\n    # Classical test predictions from full retrain\n    test_classical = cv_classical_on_test(X_train, y_tr, X_test)\n\n    # ── Step 4: Evaluate OOF ─────────────────────────────────────────────────\n    print(\"\\n[STEP 4] OOF Evaluation …\")\n\n    oof_cl_acc  = accuracy_score(y_tr, oof_classical.argmax(1))\n    oof_cnn_acc = accuracy_score(y_tr, oof_cnn.argmax(1))\n\n    # Try various ensemble weights and pick best\n    best_ens_acc = 0.0\n    best_w       = (0.5, 0.5)\n    for w_cl in np.arange(0.1, 1.0, 0.05):\n        w_cn  = 1.0 - w_cl\n        combo = w_cl * oof_classical + w_cn * oof_cnn\n        acc   = accuracy_score(y_tr, combo.argmax(1))\n        if acc > best_ens_acc:\n            best_ens_acc = acc\n            best_w       = (w_cl, w_cn)\n\n    print(f\"\\n  OOF Classical accuracy : {oof_cl_acc:.4f}\")\n    print(f\"  OOF CNN accuracy       : {oof_cnn_acc:.4f}\")\n    print(f\"  OOF Ensemble accuracy  : {best_ens_acc:.4f}  \"\n          f\"(w_classical={best_w[0]:.2f}, w_cnn={best_w[1]:.2f})\")\n\n    oof_final = best_w[0] * oof_classical + best_w[1] * oof_cnn\n    print(\"\\n  Classification Report (OOF):\")\n    print(classification_report(y_tr, oof_final.argmax(1),\n                                target_names=CLASSES, zero_division=0))\n    cm = confusion_matrix(y_tr, oof_final.argmax(1))\n    print(\"  Confusion Matrix:\")\n    print(\"         \" + \"  \".join(f\"{c:>8}\" for c in CLASSES))\n    for i, row in enumerate(cm):\n        print(f\"  {CLASSES[i]:>6}  \" + \"  \".join(f\"{v:>8}\" for v in row))\n\n    # ── Step 5: Build submission ──────────────────────────────────────────────\n    print(\"\\n[STEP 5] Building submission …\")\n    test_probs = best_w[0] * test_classical + best_w[1] * test_cnn\n    idx2cls    = {i: c for i, c in enumerate(CLASSES)}\n\n    rows = []\n    for stem, prob in zip(stems_vl, test_probs):\n        rows.append({\"Id\": stem + \".png\",\n                     \"Category\": idx2cls[int(prob.argmax())]})\n    sub = pd.DataFrame(rows)\n    sub.to_csv(SUBMIT_CSV, index=False)\n\n    print(f\"\\n  {len(sub)} predictions → {SUBMIT_CSV}\")\n    print(\"  Distribution:\")\n    print(sub[\"Category\"].value_counts().to_string())\n\n    # ── Plot confusion matrix ─────────────────────────────────────────────────\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    for ax, mat, title in zip(\n        axes,\n        [confusion_matrix(y_tr, oof_classical.argmax(1)),\n         confusion_matrix(y_tr, oof_cnn.argmax(1)),\n         cm],\n        [\"Classical OOF\", \"CNN OOF\", \"Ensemble OOF\"]\n    ):\n        im = ax.imshow(mat, cmap=\"Blues\")\n        ax.set_xticks(range(N_CLS)); ax.set_xticklabels(CLASSES, rotation=30)\n        ax.set_yticks(range(N_CLS)); ax.set_yticklabels(CLASSES)\n        ax.set_title(f\"{title}\\nacc={accuracy_score(y_tr, (oof_classical if 'Classical' in title else oof_cnn if 'CNN' in title else oof_final).argmax(1)):.4f}\")\n        for r in range(N_CLS):\n            for c_ in range(N_CLS):\n                ax.text(c_, r, str(mat[r,c_]), ha=\"center\", va=\"center\",\n                        color=\"white\" if mat[r,c_] > mat.max()/2 else \"black\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrices.png\"), dpi=100)\n    plt.close()\n\n    elapsed = (time.time() - t0) / 60\n    print(f\"\\n{'='*60}\")\n    print(f\"  OOF Accuracy  : {best_ens_acc:.4f}\")\n    print(f\"  Runtime       : {elapsed:.1f} min\")\n    print(f\"  Submission    : {SUBMIT_CSV}\")\n    print(f\"{'='*60}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T06:32:44.437608Z","iopub.execute_input":"2026-02-25T06:32:44.438297Z","iopub.status.idle":"2026-02-25T07:25:03.648642Z","shell.execute_reply.started":"2026-02-25T06:32:44.438264Z","shell.execute_reply":"2026-02-25T07:25:03.647782Z"}},"outputs":[{"name":"stdout","text":"[DEVICE] Tesla T4\n\n============================================================\n  ICPR 2026 Wheat Disease  —  v6 SPATIAL TEXTURE + CNN\n============================================================\n\n[STEP 1] Extracting spatial texture features from TRAIN …\n  [train] 600 files\n    200/600\n    400/600\n    600/600\n  MS features: (600, 120)  HS features: (600, 612)\n  Label distribution: {np.int32(0): np.int64(200), np.int32(1): np.int64(200), np.int32(2): np.int64(200)}\n\n[STEP 1b] Extracting features from competition VAL …\n  [val] 300 files\n    200/300\n\n[STEP 2] Concatenating MS+HS texture features …\n  Train feature matrix: (600, 732)\n  Test  feature matrix: (300, 732)\n\n[STEP 3] 5-Fold Cross-Validation …\n\n───────────────────────────────────────────────────────\n  FOLD 1/5\n───────────────────────────────────────────────────────\n\n  [Classical ML]\n    svm_rbf      val_acc=0.5833\n    svm_c100     val_acc=0.5167\n    rf           val_acc=0.5667\n    et           val_acc=0.5583\n    xgb          val_acc=0.5750\n    gb           val_acc=0.5750\n    ENSEMBLE     val_acc=0.5833\n\n  [CNN EfficientNet-B3]\nDownloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 47.2M/47.2M [00:00<00:00, 169MB/s] \n","output_type":"stream"},{"name":"stdout","text":"      ep 010 val_acc=0.4667  best=0.5333\n      ep 020 val_acc=0.4833  best=0.5833\n      Early stop at ep 26. Best val acc=0.5833\n    CNN best val_acc=0.5833\n\n───────────────────────────────────────────────────────\n  FOLD 2/5\n───────────────────────────────────────────────────────\n\n  [Classical ML]\n    svm_rbf      val_acc=0.4917\n    svm_c100     val_acc=0.5250\n    rf           val_acc=0.5167\n    et           val_acc=0.5167\n    xgb          val_acc=0.5667\n    gb           val_acc=0.5333\n    ENSEMBLE     val_acc=0.5583\n\n  [CNN EfficientNet-B3]\n      ep 010 val_acc=0.4333  best=0.5833\n      ep 020 val_acc=0.5000  best=0.6083\n      ep 030 val_acc=0.5583  best=0.6083\n      Early stop at ep 33. Best val acc=0.6083\n    CNN best val_acc=0.6083\n\n───────────────────────────────────────────────────────\n  FOLD 3/5\n───────────────────────────────────────────────────────\n\n  [Classical ML]\n    svm_rbf      val_acc=0.6500\n    svm_c100     val_acc=0.5333\n    rf           val_acc=0.6000\n    et           val_acc=0.5750\n    xgb          val_acc=0.5750\n    gb           val_acc=0.5750\n    ENSEMBLE     val_acc=0.5667\n\n  [CNN EfficientNet-B3]\n      ep 010 val_acc=0.5750  best=0.6167\n      ep 020 val_acc=0.5000  best=0.6333\n      ep 030 val_acc=0.4750  best=0.6500\n      Early stop at ep 38. Best val acc=0.6500\n    CNN best val_acc=0.6500\n\n───────────────────────────────────────────────────────\n  FOLD 4/5\n───────────────────────────────────────────────────────\n\n  [Classical ML]\n    svm_rbf      val_acc=0.5750\n    svm_c100     val_acc=0.5500\n    rf           val_acc=0.5500\n    et           val_acc=0.5250\n    xgb          val_acc=0.5500\n    gb           val_acc=0.5500\n    ENSEMBLE     val_acc=0.5583\n\n  [CNN EfficientNet-B3]\n      ep 010 val_acc=0.5167  best=0.5167\n      ep 020 val_acc=0.5583  best=0.5583\n      ep 030 val_acc=0.4833  best=0.5833\n      ep 040 val_acc=0.5333  best=0.5833\n      Early stop at ep 41. Best val acc=0.5833\n    CNN best val_acc=0.5833\n\n───────────────────────────────────────────────────────\n  FOLD 5/5\n───────────────────────────────────────────────────────\n\n  [Classical ML]\n    svm_rbf      val_acc=0.5333\n    svm_c100     val_acc=0.5500\n    rf           val_acc=0.5167\n    et           val_acc=0.5667\n    xgb          val_acc=0.5250\n    gb           val_acc=0.5333\n    ENSEMBLE     val_acc=0.5667\n\n  [CNN EfficientNet-B3]\n      ep 010 val_acc=0.4083  best=0.5833\n      Early stop at ep 18. Best val acc=0.5833\n    CNN best val_acc=0.5833\n\n  [Classical — final retrain on all 600 samples]\n    svm_rbf      train_acc=0.8183\n    svm_c100     train_acc=0.9817\n    rf           train_acc=0.9850\n    et           train_acc=0.9850\n    xgb          train_acc=0.9850\n    gb           train_acc=0.9850\n\n[STEP 4] OOF Evaluation …\n\n  OOF Classical accuracy : 0.5667\n  OOF CNN accuracy       : 0.6017\n  OOF Ensemble accuracy  : 0.6167  (w_classical=0.25, w_cnn=0.75)\n\n  Classification Report (OOF):\n              precision    recall  f1-score   support\n\n      Health       0.65      0.24      0.35       200\n        Rust       0.55      0.84      0.66       200\n       Other       0.71      0.77      0.73       200\n\n    accuracy                           0.62       600\n   macro avg       0.63      0.62      0.58       600\nweighted avg       0.63      0.62      0.58       600\n\n  Confusion Matrix:\n           Health      Rust     Other\n  Health        48       106        46\n    Rust        13       169        18\n   Other        13        34       153\n\n[STEP 5] Building submission …\n\n  300 predictions → /kaggle/working/submission.csv\n  Distribution:\nCategory\nRust      175\nOther      97\nHealth     28\n\n============================================================\n  OOF Accuracy  : 0.6167\n  Runtime       : 52.2 min\n  Submission    : /kaggle/working/submission.csv\n============================================================\n","output_type":"stream"}],"execution_count":2}]}