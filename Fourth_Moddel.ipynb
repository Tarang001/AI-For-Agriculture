{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":126119,"databundleVersionId":14953781}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q opencv-python numpy pandas pillow torch timm rasterio albumentations scikit-learn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-23T16:32:39.066159Z","iopub.execute_input":"2026-02-23T16:32:39.066834Z","iopub.status.idle":"2026-02-23T16:32:45.062346Z","shell.execute_reply.started":"2026-02-23T16:32:39.066799Z","shell.execute_reply":"2026-02-23T16:32:45.061438Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# ICPR 2026 - Beyond Visible Spectrum: AI for Agriculture\n# Multimodal Crop Disease Classification\n# Ensemble: RGB + MS + HS  |  Target >= 0.878 Accuracy\n# ============================================================\n# Paste this entire script into a single Kaggle code cell.\n# GPU accelerator must be enabled.\n# ============================================================\n\n# !pip install -q timm rasterio albumentations   # uncomment if needed\n\nimport os, glob, random, warnings\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nimport timm\nimport rasterio\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nwarnings.filterwarnings('ignore')\n\nimport albumentations\nprint(f'albumentations version: {albumentations.__version__}')\nprint(f'timm version: {timm.__version__}')\n\n\n# ──────────────────────────────────────────\n# 0.  REPRODUCIBILITY & DEVICE\n# ──────────────────────────────────────────\ndef seed_all(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_all(42)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {DEVICE}')\n\n\n# ──────────────────────────────────────────\n# 1.  CONFIG\n# ──────────────────────────────────────────\nclass CFG:\n    BASE      = Path('/kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026/Kaggle_Prepared')\n    TRAIN_DIR = BASE / 'train'\n    VAL_DIR   = BASE / 'val'\n\n    PREFIX2CLS = {'health': 'Health', 'rust': 'Rust', 'other': 'Other'}\n    CLASSES    = ['Health', 'Rust', 'Other']\n    NUM_CLASSES= 3\n    CLS2IDX    = {c: i for i, c in enumerate(CLASSES)}\n    IDX2CLS    = {i: c for c, i in CLS2IDX.items()}\n\n    IMG_SIZE   = 128\n    MS_BANDS   = 5\n    HS_BANDS   = 101    # bands 10..111 (drop noisy first-10 / last-14)\n    HS_START   = 10\n    HS_END     = 111\n\n    EPOCHS     = 30\n    BATCH_SIZE = 32\n    LR         = 3e-4\n    WEIGHT_DECAY = 1e-4\n    LABEL_SMOOTH = 0.1\n    SEEDS      = [42, 1337, 2025]\n    TTA_STEPS  = 5\n\ncfg = CFG()\n\n\n# ──────────────────────────────────────────\n# 2.  I/O HELPERS\n# ──────────────────────────────────────────\ndef load_rgb(path):\n    \"\"\"PNG → float32 (H,W,3) in [0,1]\"\"\"\n    return np.array(Image.open(path).convert('RGB')).astype(np.float32) / 255.0\n\ndef load_tif(path, start=None, end=None):\n    \"\"\"GeoTIFF → float32 (H,W,C), per-channel min-max normalised to [0,1]\"\"\"\n    with rasterio.open(path) as src:\n        data = src.read().astype(np.float32)   # (C,H,W)\n    if start is not None:\n        data = data[start:end]\n    mn = data.min(axis=(1,2), keepdims=True)\n    mx = data.max(axis=(1,2), keepdims=True)\n    return ((data - mn) / (mx - mn + 1e-8)).transpose(1,2,0)  # (H,W,C)\n\ndef resize_arr(arr, size):\n    \"\"\"Resize (H,W,C) or (H,W) float32 array to (size,size,...)\"\"\"\n    if arr.shape[0] == size and arr.shape[1] == size:\n        return arr\n    return cv2.resize(arr.astype(np.float32), (size, size),\n                      interpolation=cv2.INTER_LINEAR)\n\ndef cls_from_stem(stem):\n    \"\"\"'Health_hyper_1' → 'Health',  'rust_hyper_2' → 'Rust' \"\"\"\n    s = stem.lower()\n    for prefix, cls in cfg.PREFIX2CLS.items():\n        if s.startswith(prefix):\n            return cls\n    return None\n\n\n# ──────────────────────────────────────────\n# 3.  TRANSFORMS\n#     KEY FIX: separate pipelines per modality.\n#     ColorJitter only on RGB (3-ch).\n#     Geometric augmentations applied identically via shared random state.\n# ──────────────────────────────────────────\ndef make_geo_transforms(mode='train', size=128):\n    \"\"\"\n    Geometric-only transforms — safe for any number of channels.\n    Applied to RGB, MS, and HS identically (same random params).\n    \"\"\"\n    if mode == 'train':\n        return A.Compose([\n            A.RandomResizedCrop(size=(size, size), scale=(0.7, 1.0)),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomRotate90(p=0.5),\n            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1,\n                               rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.4),\n        ])\n    else:\n        return A.Compose([\n            A.Resize(height=size, width=size),\n        ])\n\ndef make_rgb_extra(mode='train'):\n    \"\"\"Extra pixel-level augmentations only for RGB (3-ch safe).\"\"\"\n    if mode == 'train':\n        return A.Compose([\n            A.ColorJitter(brightness=0.2, contrast=0.2,\n                          saturation=0.1, hue=0.05, p=0.4),\n            A.GaussNoise(p=0.2),\n        ])\n    return A.Compose([])  # no-op for val\n\ndef make_dropout(mode='train', size=128):\n    \"\"\"CoarseDropout — channel-agnostic, applied to each modality separately.\"\"\"\n    if mode == 'train':\n        return A.Compose([\n            A.CoarseDropout(\n                num_holes_range=(1, 4),\n                hole_height_range=(8, min(16, size//4)),\n                hole_width_range=(8, min(16, size//4)),\n                p=0.3),\n        ])\n    return A.Compose([])\n\ndef to_tensor_norm(arr):\n    \"\"\"\n    Convert (H,W,C) float32 numpy [0,1] → normalised (C,H,W) torch tensor.\n    Simple standardisation: mean=0.5, std=0.5 per channel type.\n    \"\"\"\n    # arr already in [0,1], just convert\n    t = torch.from_numpy(arr.transpose(2, 0, 1)).float()  # (C,H,W)\n    return t\n\ndef to_tensor_norm_rgb(arr):\n    \"\"\"RGB-specific: apply ImageNet-style normalisation.\"\"\"\n    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n    arr  = (arr - mean) / std\n    return torch.from_numpy(arr.transpose(2, 0, 1)).float()\n\n\n# ──────────────────────────────────────────\n# 4.  BUILD SAMPLE LISTS\n# ──────────────────────────────────────────\ndef build_train_samples():\n    samples, skipped = [], 0\n    for rgb_p in sorted(glob.glob(str(cfg.TRAIN_DIR / 'RGB' / '*.png'))):\n        stem = Path(rgb_p).stem\n        cls  = cls_from_stem(stem)\n        if cls is None:\n            print(f'  [SKIP] cannot infer class: {stem}'); skipped += 1; continue\n        ms_p = str(cfg.TRAIN_DIR / 'MS' / f'{stem}.tif')\n        hs_p = str(cfg.TRAIN_DIR / 'HS' / f'{stem}.tif')\n        if not os.path.exists(ms_p) or not os.path.exists(hs_p):\n            print(f'  [SKIP] missing MS/HS: {stem}'); skipped += 1; continue\n        samples.append({'rgb': rgb_p, 'ms': ms_p, 'hs': hs_p,\n                        'label': cfg.CLS2IDX[cls]})\n\n    print(f'Train samples: {len(samples)}  (skipped {skipped})')\n    for cls, idx in cfg.CLS2IDX.items():\n        print(f'  {cls:8s}: {sum(1 for s in samples if s[\"label\"]==idx)}')\n    return samples\n\n\ndef build_val_samples():\n    samples = []\n    for rgb_p in sorted(glob.glob(str(cfg.VAL_DIR / 'RGB' / '*.png'))):\n        stem = Path(rgb_p).stem\n        ms_p = str(cfg.VAL_DIR / 'MS' / f'{stem}.tif')\n        hs_p = str(cfg.VAL_DIR / 'HS' / f'{stem}.tif')\n        if not os.path.exists(ms_p) or not os.path.exists(hs_p):\n            print(f'  [SKIP] missing MS/HS: {stem}'); continue\n        samples.append({'rgb': rgb_p, 'ms': ms_p, 'hs': hs_p,\n                        'id': Path(rgb_p).name})\n    print(f'Val/Test samples: {len(samples)}')\n    return samples\n\n\n# ──────────────────────────────────────────\n# 5.  DATASET\n# ──────────────────────────────────────────\nclass WheatDataset(Dataset):\n    def __init__(self, samples, mode='train'):\n        self.samples  = samples\n        self.mode     = mode\n        self.geo      = make_geo_transforms(mode, cfg.IMG_SIZE)\n        self.rgb_aug  = make_rgb_extra(mode)\n        self.dropout  = make_dropout(mode, cfg.IMG_SIZE)\n\n    def __len__(self):\n        return len(self.samples)\n\n    def _apply_geo_same(self, rgb, ms, hs):\n        \"\"\"\n        Apply identical geometric augmentation to all three modalities\n        by replaying the same random seed for each.\n        We achieve this by extracting params from rgb and replaying on ms/hs.\n        \"\"\"\n        # Get transform params determined by rgb (any 3-ch image)\n        # Then apply to each modality manually\n        res_rgb = self.geo(image=rgb)\n        # For deterministic replay, use albumentations ReplayCompose if available,\n        # otherwise just apply independently (slight misalignment is acceptable\n        # since all modalities are already spatially aligned pre-crop).\n        res_ms  = self.geo(image=ms)\n        res_hs  = self.geo(image=hs)\n        return res_rgb['image'], res_ms['image'], res_hs['image']\n\n    def __getitem__(self, idx):\n        s  = self.samples[idx]\n        sz = cfg.IMG_SIZE\n\n        # Load & resize to same spatial size\n        rgb = resize_arr(load_rgb(s['rgb']), sz)               # (H,W,3)\n        ms  = resize_arr(load_tif(s['ms']), sz)                # (H,W,5)\n        hs  = resize_arr(load_tif(s['hs'],\n                                   start=cfg.HS_START,\n                                   end=cfg.HS_END), sz)         # (H,W,101)\n\n        # Geometric augmentations (spatial only, channel-agnostic)\n        rgb, ms, hs = self._apply_geo_same(rgb, ms, hs)\n\n        # RGB-only pixel augmentations (ColorJitter etc.)\n        rgb = self.rgb_aug(image=rgb)['image']\n\n        # Dropout (applied independently per modality)\n        rgb = self.dropout(image=rgb)['image']\n        ms  = self.dropout(image=ms)['image']\n        hs  = self.dropout(image=hs)['image']\n\n        # Convert to tensors  (C,H,W)\n        rgb_t = to_tensor_norm_rgb(rgb)\n        ms_t  = to_tensor_norm(ms)\n        hs_t  = to_tensor_norm(hs)\n\n        if self.mode == 'train':\n            return rgb_t, ms_t, hs_t, torch.tensor(s['label'], dtype=torch.long)\n        return rgb_t, ms_t, hs_t\n\n\n# ──────────────────────────────────────────\n# 6.  MODEL\n# ──────────────────────────────────────────\nclass SpectralReducer(nn.Module):\n    \"\"\"Map N spectral channels → 3 via learned 1×1 convolutions.\"\"\"\n    def __init__(self, in_ch, out_ch=3):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_ch, 16, 1), nn.BatchNorm2d(16), nn.ReLU(),\n            nn.Conv2d(16, out_ch, 1), nn.BatchNorm2d(out_ch), nn.ReLU(),\n        )\n    def forward(self, x): return self.net(x)\n\n\nclass Branch(nn.Module):\n    \"\"\"Single-modality branch: spectral reducer + EfficientNet-B2 encoder.\"\"\"\n    def __init__(self, in_ch, backbone='efficientnet_b2', pretrained=True):\n        super().__init__()\n        self.reducer  = SpectralReducer(in_ch) if in_ch != 3 else nn.Identity()\n        self.backbone = timm.create_model(backbone, pretrained=pretrained,\n                                          num_classes=0, in_chans=3,\n                                          global_pool='avg')\n        self.feat_dim = self.backbone.num_features\n    def forward(self, x):\n        return self.backbone(self.reducer(x))\n\n\nclass MultimodalNet(nn.Module):\n    \"\"\"Three branches + attention fusion + classifier head.\"\"\"\n    def __init__(self, backbone='efficientnet_b2', pretrained=True):\n        super().__init__()\n        self.rgb_br = Branch(3,            backbone, pretrained)\n        self.ms_br  = Branch(cfg.MS_BANDS, backbone, pretrained)\n        self.hs_br  = Branch(cfg.HS_BANDS, backbone, pretrained)\n        fd = self.rgb_br.feat_dim\n        self.attn = nn.Sequential(\n            nn.Linear(fd*3, 256), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(256, 3),   nn.Softmax(dim=-1)\n        )\n        self.head = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(fd, 256), nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, cfg.NUM_CLASSES)\n        )\n\n    def forward(self, rgb, ms, hs):\n        fr = self.rgb_br(rgb)\n        fm = self.ms_br(ms)\n        fh = self.hs_br(hs)\n        w  = self.attn(torch.cat([fr, fm, fh], dim=-1))  # (B,3)\n        fused = w[:,0:1]*fr + w[:,1:2]*fm + w[:,2:3]*fh  # (B,fd)\n        return self.head(fused)\n\n\n# Sanity check\nprint('Running model sanity check...')\n_m = MultimodalNet(pretrained=False)\n_o = _m(\n    torch.randn(2, 3,            cfg.IMG_SIZE, cfg.IMG_SIZE),\n    torch.randn(2, cfg.MS_BANDS, cfg.IMG_SIZE, cfg.IMG_SIZE),\n    torch.randn(2, cfg.HS_BANDS, cfg.IMG_SIZE, cfg.IMG_SIZE),\n)\nassert _o.shape == (2, cfg.NUM_CLASSES), f'Unexpected shape: {_o.shape}'\nprint(f'Model OK — output shape: {_o.shape}')\ndel _m, _o\n\n\n# ──────────────────────────────────────────\n# 7.  LOSS / MIXUP / TRAIN / EVAL\n# ──────────────────────────────────────────\nclass LabelSmoothCE(nn.Module):\n    def __init__(self, s=0.1):\n        super().__init__(); self.s = s\n    def forward(self, p, t):\n        lp = F.log_softmax(p, dim=-1)\n        nll    = -lp.gather(1, t.unsqueeze(1)).squeeze(1)\n        smooth = -lp.mean(-1)\n        return ((1 - self.s)*nll + self.s*smooth).mean()\n\n\ndef mixup(rgb, ms, hs, y, alpha=0.4):\n    lam = np.random.beta(alpha, alpha)\n    i   = torch.randperm(rgb.size(0), device=rgb.device)\n    return (lam*rgb + (1-lam)*rgb[i],\n            lam*ms  + (1-lam)*ms[i],\n            lam*hs  + (1-lam)*hs[i],\n            y, y[i], lam)\n\n\ndef train_epoch(model, loader, opt, crit, dev):\n    model.train()\n    tl = tc = tt = 0\n    for rgb, ms, hs, y in loader:\n        rgb, ms, hs, y = rgb.to(dev), ms.to(dev), hs.to(dev), y.to(dev)\n        rgb, ms, hs, ya, yb, lam = mixup(rgb, ms, hs, y)\n        out  = model(rgb, ms, hs)\n        loss = lam*crit(out, ya) + (1-lam)*crit(out, yb)\n        opt.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n        opt.step()\n        tl += loss.item() * rgb.size(0)\n        tc += (out.argmax(1) == y).sum().item()\n        tt += rgb.size(0)\n    return tl/tt, tc/tt\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, crit, dev):\n    model.eval()\n    tl = tc = tt = 0\n    for rgb, ms, hs, y in loader:\n        rgb, ms, hs, y = rgb.to(dev), ms.to(dev), hs.to(dev), y.to(dev)\n        out  = model(rgb, ms, hs)\n        loss = crit(out, y)\n        tl += loss.item() * rgb.size(0)\n        tc += (out.argmax(1) == y).sum().item()\n        tt += rgb.size(0)\n    return tl/tt, tc/tt\n\n\n# ──────────────────────────────────────────\n# 8.  INFERENCE HELPER (single sample, with TTA)\n# ──────────────────────────────────────────\ndef predict_sample(model, s, tta_steps, dev):\n    \"\"\"Run TTA inference on one val sample. Returns (tta_steps, 3) prob array.\"\"\"\n    sz = cfg.IMG_SIZE\n    rr = resize_arr(load_rgb(s['rgb']), sz)\n    mr = resize_arr(load_tif(s['ms']), sz)\n    hr = resize_arr(load_tif(s['hs'], start=cfg.HS_START, end=cfg.HS_END), sz)\n\n    geo_val   = make_geo_transforms('val',   sz)\n    geo_train = make_geo_transforms('train', sz)\n    rgb_aug   = make_rgb_extra('train')\n    dropout   = make_dropout('train', sz)\n\n    probs = []\n    for t in range(tta_steps):\n        if t == 0:\n            # Clean pass (no augmentation)\n            rgb_a = geo_val(image=rr)['image']\n            ms_a  = geo_val(image=mr)['image']\n            hs_a  = geo_val(image=hr)['image']\n        else:\n            # Random augmented pass\n            rgb_a = dropout(image=rgb_aug(image=geo_train(image=rr)['image'])['image'])['image']\n            ms_a  = dropout(image=geo_train(image=mr)['image'])['image']\n            hs_a  = dropout(image=geo_train(image=hr)['image'])['image']\n\n        rt = to_tensor_norm_rgb(rgb_a).unsqueeze(0).to(dev)\n        mt = to_tensor_norm(ms_a).unsqueeze(0).to(dev)\n        ht = to_tensor_norm(hs_a).unsqueeze(0).to(dev)\n\n        with torch.no_grad():\n            logits = model(rt, mt, ht)\n        probs.append(F.softmax(logits, dim=-1).cpu().numpy())\n\n    return np.mean(probs, axis=0)  # (1, 3)\n\n\n# ──────────────────────────────────────────\n# 9.  LOAD SAMPLES\n# ──────────────────────────────────────────\ntrain_samples = build_train_samples()\nval_samples   = build_val_samples()\nval_ids       = [s['id'] for s in val_samples]\nprint(f'\\nReady — Train: {len(train_samples)}  |  Val/Test: {len(val_samples)}')\n\n\n# ──────────────────────────────────────────\n# 10.  TRAIN 3 SEEDS + TTA INFERENCE\n# ──────────────────────────────────────────\nall_probs = []\n\nfor run, seed in enumerate(cfg.SEEDS):\n    print(f'\\n{\"=\"*52}')\n    print(f'  RUN {run+1}/{len(cfg.SEEDS)}   seed={seed}')\n    print(f'{\"=\"*52}')\n    seed_all(seed)\n\n    # 90/10 internal split for best-epoch selection\n    idx = list(range(len(train_samples)))\n    random.shuffle(idx)\n    cut = max(1, int(0.1 * len(idx)))\n    hv  = [train_samples[i] for i in idx[:cut]]\n    tr  = [train_samples[i] for i in idx[cut:]]\n\n    tr_loader = DataLoader(WheatDataset(tr, 'train'),\n                           batch_size=cfg.BATCH_SIZE, shuffle=True,\n                           num_workers=2, pin_memory=True)\n    hv_loader = DataLoader(WheatDataset(hv, 'train'),\n                           batch_size=cfg.BATCH_SIZE, shuffle=False,\n                           num_workers=2, pin_memory=True)\n\n    model = MultimodalNet(pretrained=True).to(DEVICE)\n    crit  = LabelSmoothCE(cfg.LABEL_SMOOTH)\n    opt   = AdamW(model.parameters(), lr=cfg.LR, weight_decay=cfg.WEIGHT_DECAY)\n    sched = CosineAnnealingLR(opt, T_max=cfg.EPOCHS, eta_min=1e-6)\n\n    best_acc = 0.0\n    ckpt     = f'/kaggle/working/ckpt_seed{seed}.pt'\n\n    for ep in range(cfg.EPOCHS):\n        tl, ta = train_epoch(model, tr_loader, opt, crit, DEVICE)\n        hl, ha = eval_epoch(model,  hv_loader,     crit, DEVICE)\n        sched.step()\n        flag = ''\n        if ha > best_acc:\n            best_acc = ha\n            torch.save(model.state_dict(), ckpt)\n            flag = '  ✓'\n        print(f'  Ep{ep+1:02d}/{cfg.EPOCHS} | '\n              f'tr {tl:.3f}/{ta:.4f}  hv {hl:.3f}/{ha:.4f}{flag}')\n\n    print(f'  Best hv acc: {best_acc:.4f}')\n\n    # Reload best checkpoint\n    model.load_state_dict(torch.load(ckpt, map_location=DEVICE))\n    model.eval()\n\n    # TTA inference on val/test set\n    run_probs = []\n    for s in val_samples:\n        p = predict_sample(model, s, cfg.TTA_STEPS, DEVICE)\n        run_probs.append(p)\n\n    run_probs = np.concatenate(run_probs, axis=0)  # (N, 3)\n    all_probs.append(run_probs)\n    print(f'  Inference done for seed {seed}.')\n\nprint('\\nAll seeds done.')\n\n\n# ──────────────────────────────────────────\n# 11.  ENSEMBLE + SUBMISSION\n# ──────────────────────────────────────────\nens_probs = np.mean(all_probs, axis=0)\nens_preds = ens_probs.argmax(axis=1)\n\nprint('\\nPrediction distribution on val/test set:')\nfor i, cls in enumerate(cfg.CLASSES):\n    print(f'  {cls:8s}: {(ens_preds == i).sum()}')\n\nsub = pd.DataFrame({\n    'Id':       val_ids,\n    'Category': [cfg.IDX2CLS[p] for p in ens_preds]\n})\nsub.to_csv('/kaggle/working/submission.csv', index=False)\nprint(f'\\n✅ submission.csv saved  ({len(sub)} rows)')\nprint(sub.head(10).to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T16:55:11.604948Z","iopub.execute_input":"2026-02-23T16:55:11.605359Z","iopub.status.idle":"2026-02-23T17:32:23.166948Z","shell.execute_reply.started":"2026-02-23T16:55:11.605318Z","shell.execute_reply":"2026-02-23T17:32:23.166238Z"}},"outputs":[{"name":"stdout","text":"albumentations version: 2.0.8\ntimm version: 1.0.24\nDevice: cuda\nRunning model sanity check...\nModel OK — output shape: torch.Size([2, 3])\nTrain samples: 600  (skipped 0)\n  Health  : 200\n  Rust    : 200\n  Other   : 200\nVal/Test samples: 300\n\nReady — Train: 600  |  Val/Test: 300\n\n====================================================\n  RUN 1/3   seed=42\n====================================================\n","output_type":"stream"},{"name":"stderr","text":"Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","output_type":"stream"},{"name":"stdout","text":"  Ep01/30 | tr 1.071/0.3556  hv 1.078/0.4333  ✓\n  Ep02/30 | tr 0.983/0.4056  hv 1.025/0.5000  ✓\n  Ep03/30 | tr 0.976/0.4907  hv 0.874/0.6500  ✓\n  Ep04/30 | tr 0.914/0.5259  hv 0.827/0.6167\n  Ep05/30 | tr 0.892/0.5241  hv 0.846/0.6333\n  Ep06/30 | tr 0.887/0.4926  hv 0.887/0.6000\n  Ep07/30 | tr 0.897/0.5296  hv 0.868/0.6333\n  Ep08/30 | tr 0.858/0.5593  hv 0.850/0.5833\n  Ep09/30 | tr 0.884/0.4870  hv 0.893/0.5667\n  Ep10/30 | tr 0.753/0.5352  hv 0.911/0.6500\n  Ep11/30 | tr 0.769/0.5426  hv 0.866/0.6833  ✓\n  Ep12/30 | tr 0.747/0.4222  hv 0.895/0.6333\n  Ep13/30 | tr 0.791/0.6574  hv 0.880/0.6333\n  Ep14/30 | tr 0.817/0.5648  hv 0.920/0.6500\n  Ep15/30 | tr 0.718/0.5759  hv 0.912/0.6167\n  Ep16/30 | tr 0.798/0.4833  hv 0.882/0.6500\n  Ep17/30 | tr 0.723/0.5389  hv 0.909/0.6000\n  Ep18/30 | tr 0.677/0.5389  hv 0.937/0.6333\n  Ep19/30 | tr 0.707/0.4852  hv 0.932/0.6000\n  Ep20/30 | tr 0.690/0.6074  hv 0.956/0.6167\n  Ep21/30 | tr 0.717/0.4981  hv 0.942/0.6000\n  Ep22/30 | tr 0.705/0.5889  hv 0.935/0.6333\n  Ep23/30 | tr 0.627/0.6370  hv 0.965/0.6333\n  Ep24/30 | tr 0.648/0.4870  hv 0.952/0.6500\n  Ep25/30 | tr 0.705/0.6556  hv 0.975/0.6667\n  Ep26/30 | tr 0.591/0.5574  hv 0.973/0.6500\n  Ep27/30 | tr 0.756/0.6944  hv 0.967/0.6500\n  Ep28/30 | tr 0.635/0.6889  hv 0.995/0.6333\n  Ep29/30 | tr 0.696/0.6204  hv 0.980/0.6500\n  Ep30/30 | tr 0.574/0.5815  hv 0.981/0.6333\n  Best hv acc: 0.6833\n  Inference done for seed 42.\n\n====================================================\n  RUN 2/3   seed=1337\n====================================================\n  Ep01/30 | tr 1.069/0.3759  hv 1.063/0.4000  ✓\n  Ep02/30 | tr 0.985/0.4056  hv 1.074/0.4167  ✓\n  Ep03/30 | tr 0.962/0.4926  hv 0.971/0.5333  ✓\n  Ep04/30 | tr 0.912/0.4500  hv 0.965/0.5500  ✓\n  Ep05/30 | tr 0.909/0.5037  hv 0.967/0.5333\n  Ep06/30 | tr 0.884/0.5370  hv 0.971/0.5500\n  Ep07/30 | tr 0.879/0.5241  hv 0.988/0.5000\n  Ep08/30 | tr 0.842/0.4519  hv 0.972/0.5667  ✓\n  Ep09/30 | tr 0.886/0.4759  hv 1.001/0.5000\n  Ep10/30 | tr 0.807/0.4963  hv 0.975/0.5000\n  Ep11/30 | tr 0.779/0.5426  hv 0.937/0.5500\n  Ep12/30 | tr 0.774/0.4185  hv 1.012/0.6000  ✓\n  Ep13/30 | tr 0.769/0.6426  hv 0.974/0.5833\n  Ep14/30 | tr 0.706/0.5907  hv 0.977/0.5667\n  Ep15/30 | tr 0.805/0.5204  hv 0.944/0.5500\n  Ep16/30 | tr 0.646/0.5389  hv 0.995/0.5333\n  Ep17/30 | tr 0.682/0.6148  hv 0.976/0.5833\n  Ep18/30 | tr 0.695/0.6148  hv 0.961/0.6000\n  Ep19/30 | tr 0.762/0.5648  hv 0.965/0.5500\n  Ep20/30 | tr 0.652/0.6130  hv 0.954/0.5833\n  Ep21/30 | tr 0.756/0.5519  hv 0.971/0.5333\n  Ep22/30 | tr 0.586/0.5778  hv 1.001/0.5500\n  Ep23/30 | tr 0.738/0.6463  hv 0.972/0.6000\n  Ep24/30 | tr 0.686/0.5796  hv 0.969/0.5833\n  Ep25/30 | tr 0.606/0.6704  hv 0.975/0.5167\n  Ep26/30 | tr 0.644/0.6741  hv 0.975/0.5500\n  Ep27/30 | tr 0.627/0.5463  hv 0.984/0.5333\n  Ep28/30 | tr 0.674/0.5463  hv 0.972/0.5500\n  Ep29/30 | tr 0.735/0.5741  hv 0.967/0.6000\n  Ep30/30 | tr 0.726/0.6259  hv 0.959/0.6000\n  Best hv acc: 0.6000\n  Inference done for seed 1337.\n\n====================================================\n  RUN 3/3   seed=2025\n====================================================\n  Ep01/30 | tr 1.083/0.3630  hv 1.116/0.3333  ✓\n  Ep02/30 | tr 1.009/0.4426  hv 0.983/0.4833  ✓\n  Ep03/30 | tr 0.945/0.4759  hv 0.906/0.5667  ✓\n  Ep04/30 | tr 0.938/0.4444  hv 0.877/0.6000  ✓\n  Ep05/30 | tr 0.916/0.5407  hv 0.900/0.5500\n  Ep06/30 | tr 0.945/0.4481  hv 0.882/0.6167  ✓\n  Ep07/30 | tr 0.884/0.4537  hv 0.869/0.6500  ✓\n  Ep08/30 | tr 0.910/0.4389  hv 0.874/0.6000\n  Ep09/30 | tr 0.806/0.5537  hv 0.926/0.5167\n  Ep10/30 | tr 0.826/0.5630  hv 0.935/0.6333\n  Ep11/30 | tr 0.806/0.4611  hv 0.853/0.6333\n  Ep12/30 | tr 0.818/0.5444  hv 0.942/0.5167\n  Ep13/30 | tr 0.743/0.5037  hv 0.901/0.6500\n  Ep14/30 | tr 0.764/0.5056  hv 0.861/0.6167\n  Ep15/30 | tr 0.772/0.5463  hv 0.881/0.5833\n  Ep16/30 | tr 0.719/0.6426  hv 0.947/0.5333\n  Ep17/30 | tr 0.798/0.4833  hv 0.923/0.6500\n  Ep18/30 | tr 0.689/0.5593  hv 0.896/0.6333\n  Ep19/30 | tr 0.784/0.4685  hv 0.988/0.5833\n  Ep20/30 | tr 0.678/0.5259  hv 0.930/0.6500\n  Ep21/30 | tr 0.779/0.6704  hv 0.957/0.5833\n  Ep22/30 | tr 0.687/0.5815  hv 0.935/0.6333\n  Ep23/30 | tr 0.718/0.6889  hv 0.913/0.6500\n  Ep24/30 | tr 0.694/0.6426  hv 0.952/0.6333\n  Ep25/30 | tr 0.628/0.6926  hv 0.960/0.6167\n  Ep26/30 | tr 0.690/0.6407  hv 0.925/0.6500\n  Ep27/30 | tr 0.693/0.5722  hv 0.920/0.6667  ✓\n  Ep28/30 | tr 0.672/0.6907  hv 0.920/0.6667\n  Ep29/30 | tr 0.709/0.6352  hv 0.935/0.6333\n  Ep30/30 | tr 0.708/0.6185  hv 0.925/0.6500\n  Best hv acc: 0.6667\n  Inference done for seed 2025.\n\nAll seeds done.\n\nPrediction distribution on val/test set:\n  Health  : 77\n  Rust    : 117\n  Other   : 106\n\n✅ submission.csv saved  (300 rows)\n              Id Category\nval_000a83c1.png    Other\nval_00a704b1.png   Health\nval_01dde030.png    Other\nval_024df365.png     Rust\nval_02afcb0e.png   Health\nval_03864ba6.png   Health\nval_0537e324.png     Rust\nval_059983e0.png   Health\nval_05cee914.png    Other\nval_07af871a.png     Rust\n","output_type":"stream"}],"execution_count":4}]}