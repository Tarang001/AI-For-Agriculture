{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":126119,"databundleVersionId":14953781}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q timm rasterio opencv-python-headless","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-20T09:49:38.663353Z","iopub.execute_input":"2026-02-20T09:49:38.663610Z","iopub.status.idle":"2026-02-20T09:49:45.084848Z","shell.execute_reply.started":"2026-02-20T09:49:38.663576Z","shell.execute_reply":"2026-02-20T09:49:45.083712Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# ICPR 2026 – Beyond Visible Spectrum: AI for Agriculture\n# OPTIMIZED TRAINING + INFERENCE PIPELINE\n#\n# SUBMISSION FORMAT:\n#   Id          → filename WITH .png extension  (e.g. val_02afcb0e.png)\n#   Category    → Health | Rust | Other\n#\n# DATASET LAYOUT (confirmed):\n#   train/RGB/  Health_hyper_1.png | rust_hyper_2.png | other_hyper_12.png\n#   train/MS/   same stems, .tif\n#   train/HS/   same stems, .tif  (125 bands, 450–950 nm)\n#   val/RGB/    val_02afcb0e.png  (no class in filename)\n#   val/MS/     val_02afcb0e.tif\n#   val/HS/     val_02afcb0e.tif\n#\n# PASTE ENTIRE FILE INTO ONE SINGLE KAGGLE CELL AND RUN.\n# =============================================================================\n\n# ── Install timm if not present ──────────────────────────────────────────────\nimport subprocess, sys\ntry:\n    import timm\nexcept ImportError:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"timm\"])\n    import timm\n\nimport os, random, warnings\nfrom pathlib import Path\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport rasterio\nfrom rasterio.errors import NotGeoreferencedWarning\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom torch.amp import GradScaler, autocast\n\nwarnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# ── Reproducibility ──────────────────────────────────────────────────────────\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\ntorch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark     = False\n\ndevice  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nN_GPUS  = torch.cuda.device_count()\nUSE_AMP = torch.cuda.is_available()\nprint(f\"[HW] device={device}  GPUs={N_GPUS}  AMP={USE_AMP}\")\nif torch.cuda.is_available():\n    print(f\"[HW] GPU={torch.cuda.get_device_name(0)}  \"\n          f\"VRAM={torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")\n\n\n# =============================================================================\n# SECTION 1 – CONFIGURATION\n# =============================================================================\n\nclass CFG:\n    CLASSES     = [\"Health\", \"Rust\", \"Other\"]\n    NUM_CLASSES = 3\n    CLASS2IDX   = {\"Health\": 0, \"Rust\": 1, \"Other\": 2}\n    IDX2CLASS   = {0: \"Health\", 1: \"Rust\", 2: \"Other\"}\n\n    MS_BANDS  = 5\n    MS_IN_CH  = 7       # 5 bands + NDVI + RENDVI\n\n    # 40 HS bands across 4 wavelength clusters (700–860 nm)\n    # Cluster A 700–740nm idx 62–71 | B 740–780nm idx 72–81\n    # Cluster C 780–820nm idx 82–91 | D 820–860nm idx 92–101\n    HS_KEY_BANDS = (list(range(62, 72)) + list(range(72, 82)) +\n                    list(range(82, 92)) + list(range(92, 102)))\n    HS_IN_CH     = 40\n\n    IMG_SIZE     = 64\n\n    BATCH_SIZE   = 24\n    GRAD_ACCUM   = 2\n    NUM_WORKERS  = 4\n    LR           = 2e-4\n    LR_MIN       = 1e-6\n    WEIGHT_DECAY = 1e-2\n    LABEL_SMOOTH = 0.10\n    FOCAL_GAMMA  = 2.0\n    MIXUP_ALPHA  = 0.2\n    BAND_DROP_P  = 0.15\n    VAL_FRAC     = 0.15\n    WARMUP_STEPS = 100\n\n    EPOCHS_S1_MS = 15\n    EPOCHS_S1_HS = 15\n    EPOCHS_S2    = 20\n    EPOCHS_S3    = 30\n\n    SUBMISSION   = \"/kaggle/working/submission.csv\"\n\n\n# =============================================================================\n# SECTION 2 – PATH DETECTION\n# =============================================================================\n\ndef _has_split(p):\n    return (p / \"train\").is_dir() and (p / \"val\").is_dir()\n\ndef find_root():\n    hardcoded = [\n        \"/kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026/Kaggle_Prepared\",\n        \"/kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026\",\n        \"/kaggle/input/Kaggle_Prepared\",\n        \"/kaggle/working/Kaggle_Prepared\",\n        \"/kaggle/working\",\n    ]\n    for raw in hardcoded:\n        p = Path(raw)\n        if p.is_dir() and _has_split(p):\n            print(f\"[PATH] Root: {p}\")\n            return p\n    base = Path(\"/kaggle/input\")\n    if base.is_dir():\n        def _walk(d, depth):\n            if depth == 0: return None\n            try:\n                for c in sorted(d.iterdir()):\n                    if c.is_dir():\n                        if _has_split(c): return c\n                        r = _walk(c, depth - 1)\n                        if r: return r\n            except PermissionError: pass\n        found = _walk(base, 6)\n        if found:\n            print(f\"[PATH] Root (recursive): {found}\")\n            return found\n    raise FileNotFoundError(\"Cannot find dataset root with train/ and val/\")\n\nROOT      = find_root()\nTRAIN_DIR = ROOT / \"train\"\nVAL_DIR   = ROOT / \"val\"\nprint(f\"[PATH] TRAIN={TRAIN_DIR}\")\nprint(f\"[PATH] VAL  ={VAL_DIR}\")\n\n\n# =============================================================================\n# SECTION 3 – SAMPLE COLLECTION\n# =============================================================================\n\n_CLS_MAP = {c.lower(): c for c in CFG.CLASSES}\n\ndef class_from_stem(stem):\n    \"\"\"Extract class from filename prefix: 'Health_hyper_1' → ('Health', 0)\"\"\"\n    prefix = stem.split(\"_\")[0].lower()\n    canon  = _CLS_MAP.get(prefix)\n    if canon is None:\n        return None, -1\n    return canon, CFG.CLASS2IDX[canon]\n\n\ndef collect_train(split_dir):\n    rgb_dir = split_dir / \"RGB\"\n    ms_dir  = split_dir / \"MS\"\n    hs_dir  = split_dir / \"HS\"\n    pngs    = sorted(rgb_dir.glob(\"*.png\")) + sorted(rgb_dir.glob(\"*.PNG\"))\n    print(f\"[COLLECT] Train: {len(pngs)} PNG files\")\n    records, per_cls = [], defaultdict(int)\n    for png in pngs:\n        stem = png.stem\n        canon, label = class_from_stem(stem)\n        if canon is None:\n            continue\n        per_cls[canon] += 1\n        records.append({\n            \"stem\":     stem,\n            \"filename\": png.name,          # stem + .png extension\n            \"ms_path\":  ms_dir / (stem + \".tif\"),\n            \"hs_path\":  hs_dir / (stem + \".tif\"),\n            \"label\":    label,\n            \"cls_name\": canon,\n        })\n    print(f\"[COLLECT] Train records={len(records)}  dist={dict(per_cls)}\")\n    return records\n\n\ndef collect_val(split_dir):\n    rgb_dir = split_dir / \"RGB\"\n    ms_dir  = split_dir / \"MS\"\n    hs_dir  = split_dir / \"HS\"\n    pngs    = sorted(rgb_dir.glob(\"*.png\")) + sorted(rgb_dir.glob(\"*.PNG\"))\n    print(f\"[COLLECT] Val: {len(pngs)} PNG files\")\n    records = []\n    for png in pngs:\n        stem = png.stem\n        records.append({\n            \"stem\":     stem,\n            \"filename\": png.name,          # e.g. val_02afcb0e.png\n            \"ms_path\":  ms_dir / (stem + \".tif\"),\n            \"hs_path\":  hs_dir / (stem + \".tif\"),\n            \"label\":    -1,\n            \"cls_name\": \"__val__\",\n        })\n    print(f\"[COLLECT] Val records={len(records)}\")\n    return records\n\n\ntrain_records = collect_train(TRAIN_DIR)\nval_records   = collect_val(VAL_DIR)\n\n# Label integrity check\nbad = [r for r in train_records if r[\"label\"] not in {0, 1, 2}]\nif bad:\n    raise ValueError(f\"{len(bad)} train records with invalid label: {bad[0]}\")\nif len(train_records) == 0:\n    raise RuntimeError(\"Zero training records. Check filenames like 'Health_hyper_1.png'\")\nif len(val_records) == 0:\n    raise RuntimeError(\"Zero val records found.\")\nprint(f\"[OK] train={len(train_records)}  val={len(val_records)}\")\n\n# Class weights for focal loss\ncls_counts  = defaultdict(int)\nfor r in train_records: cls_counts[r[\"label\"]] += 1\ncls_weights = torch.tensor(\n    [len(train_records) / (CFG.NUM_CLASSES * cls_counts.get(i, 1))\n     for i in range(CFG.NUM_CLASSES)], dtype=torch.float32)\nprint(f\"[INFO] Class weights: {cls_weights.numpy().round(3)}\")\n\n\n# =============================================================================\n# SECTION 4 – DATA LOADING\n# =============================================================================\n\ndef load_ms(path):\n    try:\n        with rasterio.open(str(path)) as src:\n            data = src.read().astype(np.float32)\n    except Exception:\n        return torch.zeros(CFG.MS_BANDS, CFG.IMG_SIZE, CFG.IMG_SIZE)\n    for b in range(data.shape[0]):\n        lo, hi = np.percentile(data[b], 1), np.percentile(data[b], 99)\n        data[b] = np.clip((data[b] - lo) / (hi - lo + 1e-8), 0., 1.)\n    out = np.stack([cv2.resize(data[b], (CFG.IMG_SIZE, CFG.IMG_SIZE),\n                               interpolation=cv2.INTER_LINEAR)\n                    for b in range(data.shape[0])])\n    return torch.from_numpy(out)\n\n\ndef load_hs(path, band_dropout=False):\n    bands_1idx = [b + 1 for b in CFG.HS_KEY_BANDS]\n    try:\n        with rasterio.open(str(path)) as src:\n            data = src.read(bands_1idx).astype(np.float32)\n    except Exception:\n        return torch.zeros(CFG.HS_IN_CH, CFG.IMG_SIZE, CFG.IMG_SIZE)\n    for b in range(data.shape[0]):\n        lo, hi = np.percentile(data[b], 1), np.percentile(data[b], 99)\n        data[b] = np.clip((data[b] - lo) / (hi - lo + 1e-8), 0., 1.)\n    if band_dropout and random.random() < CFG.BAND_DROP_P:\n        data[random.randint(0, data.shape[0] - 1)] = 0.0\n    out = np.stack([cv2.resize(data[b], (CFG.IMG_SIZE, CFG.IMG_SIZE),\n                               interpolation=cv2.INTER_LINEAR)\n                    for b in range(data.shape[0])])\n    return torch.from_numpy(out)\n\n\ndef inject_vi(ms):\n    \"\"\"(B,5,H,W) → (B,7,H,W) by appending NDVI and RENDVI channels.\"\"\"\n    RED    = ms[:, 2:3]\n    RE     = ms[:, 3:4]\n    NIR    = ms[:, 4:5]\n    NDVI   = (NIR - RED) / (NIR + RED + 1e-8)\n    RENDVI = (NIR - RE)  / (NIR + RE  + 1e-8)\n    return torch.cat([ms, (NDVI+1)/2, (RENDVI+1)/2], dim=1)\n\n\n# =============================================================================\n# SECTION 5 – DATASET\n# =============================================================================\n\nclass CropDS(Dataset):\n    def __init__(self, records, augment=False):\n        self.records = [r for r in records\n                        if r[\"label\"] == -1 or r[\"label\"] in {0, 1, 2}]\n        self.augment = augment\n\n    def __len__(self): return len(self.records)\n\n    def _aug(self, ms, hs):\n        if random.random() > 0.5: ms, hs = torch.flip(ms,[-1]), torch.flip(hs,[-1])\n        if random.random() > 0.5: ms, hs = torch.flip(ms,[-2]), torch.flip(hs,[-2])\n        k = random.randint(0, 3)\n        if k:\n            ms = torch.rot90(ms, k, [-2,-1])\n            hs = torch.rot90(hs, k, [-2,-1])\n        return ms, hs\n\n    def __getitem__(self, idx):\n        r  = self.records[idx]\n        ms = load_ms(r[\"ms_path\"])\n        hs = load_hs(r[\"hs_path\"], band_dropout=self.augment)\n        if self.augment:\n            ms, hs = self._aug(ms, hs)\n        return {\n            \"ms\":       ms,\n            \"hs\":       hs,\n            \"label\":    torch.tensor(r[\"label\"], dtype=torch.long),\n            \"stem\":     r[\"stem\"],\n            \"filename\": r[\"filename\"],     # includes .png extension\n        }\n\n\ndef mixup_batch(ms, hs, labels, alpha=CFG.MIXUP_ALPHA):\n    lam  = np.random.beta(alpha, alpha) if alpha > 0 else 1.0\n    idx  = torch.randperm(ms.size(0), device=ms.device)\n    return lam*ms + (1-lam)*ms[idx], lam*hs + (1-lam)*hs[idx], labels, labels[idx], lam\n\n\n# =============================================================================\n# SECTION 6 – MODELS\n# =============================================================================\n\nclass SESpectralAttention(nn.Module):\n    \"\"\"Squeeze-and-Excitation block for spectral channel reweighting.\"\"\"\n    def __init__(self, n_ch, reduction=4):\n        super().__init__()\n        hidden = max(n_ch // reduction, 4)\n        self.net = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n            nn.Linear(n_ch, hidden), nn.ReLU(inplace=True),\n            nn.Linear(hidden, n_ch), nn.Sigmoid(),\n        )\n    def forward(self, x):\n        return x * self.net(x).unsqueeze(-1).unsqueeze(-1)\n\n\nclass GatedFusion(nn.Module):\n    \"\"\"Learns per-sample dynamic importance of MS vs HS features.\"\"\"\n    def __init__(self, feat_dim=256):\n        super().__init__()\n        self.gate = nn.Sequential(\n            nn.Linear(feat_dim*2, feat_dim), nn.ReLU(inplace=True),\n            nn.Linear(feat_dim, 2), nn.Softmax(dim=-1),\n        )\n        self.proj = nn.Linear(feat_dim, feat_dim)\n    def forward(self, ms_f, hs_f):\n        w = self.gate(torch.cat([ms_f, hs_f], dim=1))\n        return self.proj(w[:,0:1]*ms_f + w[:,1:2]*hs_f)\n\n\ndef _make_efficientnet(in_ch):\n    model   = timm.create_model(\"efficientnet_b0\", pretrained=True,\n                                 num_classes=0, global_pool=\"avg\")\n    orig_w  = model.conv_stem.weight.data          # (32, 3, 3, 3)\n    avg_w   = orig_w.mean(dim=1, keepdim=True)     # (32, 1, 3, 3)\n    new_w   = avg_w.repeat(1, in_ch, 1, 1) * (3.0 / in_ch)\n    model.conv_stem = nn.Conv2d(in_ch, 32, 3, stride=2, padding=1, bias=False)\n    model.conv_stem.weight = nn.Parameter(new_w)\n    return model, model.num_features               # 1280 for B0\n\n\nclass MSEncoder(nn.Module):\n    def __init__(self, feat_dim=256):\n        super().__init__()\n        self.backbone, out_dim = _make_efficientnet(CFG.MS_IN_CH)\n        self.proj = nn.Sequential(nn.Linear(out_dim, feat_dim),\n                                  nn.LayerNorm(feat_dim), nn.GELU(), nn.Dropout(0.3))\n    def forward(self, x): return self.proj(self.backbone(x))\n    def freeze(self):\n        for p in self.backbone.parameters(): p.requires_grad = False\n    def unfreeze_top(self, n=4):\n        for blk in list(self.backbone.blocks)[-n:]:\n            for p in blk.parameters(): p.requires_grad = True\n        for p in self.backbone.conv_head.parameters(): p.requires_grad = True\n        for p in self.backbone.bn2.parameters():       p.requires_grad = True\n    def unfreeze_all(self):\n        for p in self.backbone.parameters(): p.requires_grad = True\n\n\nclass HSEncoder(nn.Module):\n    def __init__(self, feat_dim=256):\n        super().__init__()\n        self.se_attn  = SESpectralAttention(CFG.HS_IN_CH, reduction=4)\n        self.backbone, out_dim = _make_efficientnet(CFG.HS_IN_CH)\n        self.proj = nn.Sequential(nn.Linear(out_dim, feat_dim),\n                                  nn.LayerNorm(feat_dim), nn.GELU(), nn.Dropout(0.3))\n    def forward(self, x): return self.proj(self.backbone(self.se_attn(x)))\n    def freeze(self):\n        for p in self.backbone.parameters(): p.requires_grad = False\n    def unfreeze_top(self, n=4):\n        for blk in list(self.backbone.blocks)[-n:]:\n            for p in blk.parameters(): p.requires_grad = True\n        for p in self.backbone.conv_head.parameters(): p.requires_grad = True\n        for p in self.backbone.bn2.parameters():       p.requires_grad = True\n    def unfreeze_all(self):\n        for p in self.backbone.parameters(): p.requires_grad = True\n\n\nclass MSModel(nn.Module):\n    def __init__(self, feat_dim=256):\n        super().__init__()\n        self.encoder = MSEncoder(feat_dim)\n        self.clf      = nn.Linear(feat_dim, CFG.NUM_CLASSES)\n    def forward(self, ms, hs=None): return self.clf(self.encoder(ms))\n\n\nclass HSModel(nn.Module):\n    def __init__(self, feat_dim=256):\n        super().__init__()\n        self.encoder = HSEncoder(feat_dim)\n        self.clf      = nn.Linear(feat_dim, CFG.NUM_CLASSES)\n    def forward(self, ms=None, hs=None): return self.clf(self.encoder(hs))\n\n\nclass FusionModel(nn.Module):\n    def __init__(self, feat_dim=256, ms_w=None, hs_w=None):\n        super().__init__()\n        self.ms_enc = MSEncoder(feat_dim)\n        self.hs_enc = HSEncoder(feat_dim)\n        if ms_w: self.ms_enc.load_state_dict(ms_w, strict=False)\n        if hs_w: self.hs_enc.load_state_dict(hs_w, strict=False)\n        self.fusion = GatedFusion(feat_dim)\n        self.clf    = nn.Sequential(\n            nn.Linear(feat_dim, 128), nn.GELU(), nn.Dropout(0.4),\n            nn.Linear(128, CFG.NUM_CLASSES),\n        )\n    def forward(self, ms, hs):\n        return self.clf(self.fusion(self.ms_enc(ms), self.hs_enc(hs)))\n\n\n# =============================================================================\n# SECTION 7 – LOSS\n# =============================================================================\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=CFG.FOCAL_GAMMA, weights=None, smooth=CFG.LABEL_SMOOTH):\n        super().__init__()\n        self.gamma  = gamma\n        self.smooth = smooth\n        self.register_buffer(\"weights\", weights)\n\n    def forward(self, logits, targets):\n        targets = targets.clamp(0, CFG.NUM_CLASSES - 1)\n        log_p   = F.log_softmax(logits, dim=-1)\n        p       = log_p.exp()\n        with torch.no_grad():\n            t = torch.full_like(log_p, self.smooth / (CFG.NUM_CLASSES - 1))\n            t.scatter_(1, targets.long().unsqueeze(1), 1.0 - self.smooth)\n        p_true  = (p * t).sum(dim=-1, keepdim=True)\n        focal_w = (1.0 - p_true).pow(self.gamma)\n        loss    = -(t * log_p).sum(dim=-1) * focal_w.squeeze(-1)\n        if self.weights is not None:\n            loss = loss * self.weights[targets.long()]\n        return loss.mean()\n\n\ndef mixup_loss(criterion, logits, la, lb, lam):\n    return lam * criterion(logits, la) + (1 - lam) * criterion(logits, lb)\n\n\n# =============================================================================\n# SECTION 8 – TRAINING LOOP\n# =============================================================================\n\ndef warmup_lr(optimizer, step, warmup_steps, base_lr):\n    if step < warmup_steps:\n        lr = base_lr * step / max(warmup_steps, 1)\n        for pg in optimizer.param_groups: pg[\"lr\"] = lr\n\n\ndef _fwd(model, ms, hs, mode):\n    ms_vi = inject_vi(ms)\n    if   mode == \"ms\":     return model(ms=ms_vi)\n    elif mode == \"hs\":     return model(hs=hs)\n    else:                  return model(ms=ms_vi, hs=hs)\n\n\ndef _filter(ms, hs, label):\n    mask = (label >= 0) & (label < CFG.NUM_CLASSES)\n    return ms[mask], hs[mask], label[mask]\n\n\ndef train_epoch(model, loader, optimizer, criterion, scaler, mode,\n                use_mixup, grad_accum, g_step, warmup_steps, base_lr):\n    model.train()\n    loss_sum = correct = total = 0\n    optimizer.zero_grad(set_to_none=True)\n\n    for i, batch in enumerate(loader):\n        ms    = batch[\"ms\"].to(device, non_blocking=True)\n        hs    = batch[\"hs\"].to(device, non_blocking=True)\n        label = batch[\"label\"].to(device, non_blocking=True)\n        ms, hs, label = _filter(ms, hs, label)\n        if label.numel() == 0: continue\n\n        warmup_lr(optimizer, g_step, warmup_steps, base_lr)\n        g_step += 1\n\n        with autocast('cuda', enabled=USE_AMP):\n            if use_mixup:\n                ms_m, hs_m, la, lb, lam = mixup_batch(ms, hs, label)\n                out  = _fwd(model, ms_m, hs_m, mode)\n                loss = mixup_loss(criterion, out, la, lb, lam) / grad_accum\n            else:\n                out  = _fwd(model, ms, hs, mode)\n                loss = criterion(out, label) / grad_accum\n\n        scaler.scale(loss).backward()\n\n        if (i + 1) % grad_accum == 0 or (i + 1) == len(loader):\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer); scaler.update()\n            optimizer.zero_grad(set_to_none=True)\n\n        loss_sum += loss.item() * grad_accum * label.size(0)\n        with torch.no_grad():\n            correct += (out.argmax(1) == label).sum().item()\n        total += label.size(0)\n\n    return loss_sum / max(total,1), correct / max(total,1), g_step\n\n\n@torch.no_grad()\ndef eval_epoch(model, loader, criterion, mode):\n    model.eval()\n    loss_sum = correct = total = 0\n    per_c = defaultdict(int); per_t = defaultdict(int)\n    for batch in loader:\n        ms    = batch[\"ms\"].to(device, non_blocking=True)\n        hs    = batch[\"hs\"].to(device, non_blocking=True)\n        label = batch[\"label\"].to(device, non_blocking=True)\n        ms, hs, label = _filter(ms, hs, label)\n        if label.numel() == 0: continue\n        with autocast('cuda', enabled=USE_AMP):\n            out  = _fwd(model, ms, hs, mode)\n            loss = criterion(out, label)\n        loss_sum += loss.item() * label.size(0)\n        preds     = out.argmax(1)\n        correct  += (preds == label).sum().item()\n        total    += label.size(0)\n        for lbl in range(CFG.NUM_CLASSES):\n            m = label == lbl\n            per_c[lbl] += (preds[m] == lbl).sum().item()\n            per_t[lbl] += m.sum().item()\n    cls_str = \" | \".join(f\"{CFG.IDX2CLASS[k]}={per_c[k]/max(per_t[k],1):.3f}\"\n                         for k in sorted(CFG.IDX2CLASS))\n    return loss_sum / max(total,1), correct / max(total,1), cls_str\n\n\ndef run_stage(model, tr_ld, va_ld, epochs, lr, mode, ckpt,\n              use_mixup=False, warmup_steps=CFG.WARMUP_STEPS):\n    cw        = cls_weights.to(device)\n    criterion = FocalLoss(gamma=CFG.FOCAL_GAMMA, weights=cw,\n                          smooth=CFG.LABEL_SMOOTH).to(device)\n    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n                      lr=lr, weight_decay=CFG.WEIGHT_DECAY)\n    scheduler = CosineAnnealingWarmRestarts(optimizer,\n                                            T_0=max(1, epochs // 2),\n                                            eta_min=CFG.LR_MIN)\n    scaler    = GradScaler('cuda', enabled=USE_AMP)\n    best_acc  = 0.0; best_wts = None; g_step = 0\n\n    print(f\"\\n{'─'*65}\")\n    print(f\"  mode={mode.upper()}  epochs={epochs}  lr={lr:.0e}  mixup={use_mixup}\")\n    print(f\"{'─'*65}\")\n\n    for ep in range(1, epochs + 1):\n        tr_l, tr_a, g_step = train_epoch(model, tr_ld, optimizer, criterion,\n                                          scaler, mode, use_mixup,\n                                          CFG.GRAD_ACCUM, g_step,\n                                          warmup_steps, lr)\n        va_l, va_a, cls_str = eval_epoch(model, va_ld, criterion, mode)\n        if g_step >= warmup_steps: scheduler.step(ep)\n\n        print(f\"  ep {ep:3d}/{epochs}  tr {tr_l:.4f}/{tr_a:.3f}  \"\n              f\"va {va_l:.4f}/{va_a:.3f}  lr {optimizer.param_groups[0]['lr']:.2e}\")\n        print(f\"    {cls_str}\")\n\n        if va_a > best_acc:\n            best_acc = va_a\n            best_wts = {k: v.clone() for k, v in model.state_dict().items()}\n            torch.save(best_wts, ckpt)\n            print(f\"    ✓ checkpoint saved  val_acc={va_a:.4f}\")\n\n    if best_wts: model.load_state_dict(best_wts)\n    print(f\"  Stage best val_acc: {best_acc:.4f}\")\n    return model, best_acc\n\n\n# =============================================================================\n# SECTION 9 – DATALOADER SETUP\n# =============================================================================\n\nrandom.shuffle(train_records)\nbuckets = defaultdict(list)\nfor r in train_records: buckets[r[\"label\"]].append(r)\n\nint_train, int_val = [], []\nfor lbl, recs in buckets.items():\n    n_val = max(1, int(len(recs) * CFG.VAL_FRAC))\n    int_val.extend(recs[:n_val]); int_train.extend(recs[n_val:])\nrandom.shuffle(int_train); random.shuffle(int_val)\nprint(f\"\\n[DS] int_train={len(int_train)}  int_val={len(int_val)}  test={len(val_records)}\")\n\ntrain_ds = CropDS(int_train,   augment=True)\nval_ds   = CropDS(int_val,     augment=False)\ntest_ds  = CropDS(val_records, augment=False)\n\nkw = dict(num_workers=CFG.NUM_WORKERS, pin_memory=True, persistent_workers=True)\ntrain_ld = DataLoader(train_ds, CFG.BATCH_SIZE, shuffle=True,  drop_last=True,  **kw)\nval_ld   = DataLoader(val_ds,   CFG.BATCH_SIZE, shuffle=False, drop_last=False, **kw)\ntest_ld  = DataLoader(test_ds,  CFG.BATCH_SIZE, shuffle=False, drop_last=False, **kw)\nprint(f\"[DS] train={len(train_ld)} val={len(val_ld)} test={len(test_ld)} batches\")\n\n\n# =============================================================================\n# SECTION 10 – STAGE 1a: MS BASELINE\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*65)\nprint(\"  STAGE 1a – MS Baseline (EfficientNet-B0, 7ch, backbone frozen)\")\nprint(\"=\"*65)\n\nmodel_ms = MSModel(feat_dim=256).to(device)\nmodel_ms.encoder.freeze()\nif N_GPUS > 1: model_ms = nn.DataParallel(model_ms)\n\nmodel_ms, acc_s1a = run_stage(model_ms, train_ld, val_ld,\n                               CFG.EPOCHS_S1_MS, CFG.LR * 3, \"ms\",\n                               \"/kaggle/working/ckpt_ms_frozen.pth\")\n\nprint(\"\\n  [1a] Unfreezing top-4 backbone blocks…\")\n(model_ms.module if isinstance(model_ms, nn.DataParallel) else model_ms).encoder.unfreeze_top(4)\nmodel_ms, acc_s1a = run_stage(model_ms, train_ld, val_ld,\n                               10, CFG.LR / 3, \"ms\",\n                               \"/kaggle/working/ckpt_ms.pth\",\n                               warmup_steps=50)\nprint(f\"[S1a DONE] val_acc={acc_s1a:.4f}\")\n\n\n# =============================================================================\n# SECTION 11 – STAGE 1b: HS CORE MODEL\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*65)\nprint(\"  STAGE 1b – HS Core (EfficientNet-B0, 40ch, SE-attention, frozen)\")\nprint(\"=\"*65)\n\nmodel_hs = HSModel(feat_dim=256).to(device)\nmodel_hs.encoder.freeze()\nif N_GPUS > 1: model_hs = nn.DataParallel(model_hs)\n\nmodel_hs, acc_s1b = run_stage(model_hs, train_ld, val_ld,\n                               CFG.EPOCHS_S1_HS, CFG.LR * 3, \"hs\",\n                               \"/kaggle/working/ckpt_hs_frozen.pth\")\n\nprint(\"\\n  [1b] Unfreezing top-4 backbone blocks…\")\n(model_hs.module if isinstance(model_hs, nn.DataParallel) else model_hs).encoder.unfreeze_top(4)\nmodel_hs, acc_s1b = run_stage(model_hs, train_ld, val_ld,\n                               10, CFG.LR / 3, \"hs\",\n                               \"/kaggle/working/ckpt_hs.pth\",\n                               warmup_steps=50)\nprint(f\"[S1b DONE] val_acc={acc_s1b:.4f}\")\n\n\n# =============================================================================\n# SECTION 12 – STAGE 2: GATED FUSION (partial unfreeze)\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*65)\nprint(\"  STAGE 2 – Gated Fusion (top-4 blocks unfrozen)\")\nprint(\"=\"*65)\n\ndef _enc_w(m):\n    core = m.module if isinstance(m, nn.DataParallel) else m\n    return core.encoder.state_dict() if hasattr(core, \"encoder\") else None\n\nmodel_f = FusionModel(feat_dim=256,\n                      ms_w=_enc_w(model_ms),\n                      hs_w=_enc_w(model_hs)).to(device)\n\n# Freeze full backbones, keep SE-attn + proj + top-4 blocks trainable\nmodel_f.ms_enc.freeze(); model_f.hs_enc.freeze()\nmodel_f.ms_enc.unfreeze_top(4); model_f.hs_enc.unfreeze_top(4)\nfor p in model_f.ms_enc.proj.parameters():      p.requires_grad = True\nfor p in model_f.hs_enc.proj.parameters():      p.requires_grad = True\nfor p in model_f.hs_enc.se_attn.parameters():   p.requires_grad = True\n\nif N_GPUS > 1: model_f = nn.DataParallel(model_f)\n\nmodel_f, acc_s2 = run_stage(model_f, train_ld, val_ld,\n                              CFG.EPOCHS_S2, CFG.LR, \"fusion\",\n                              \"/kaggle/working/ckpt_fusion_s2.pth\")\nprint(f\"[S2 DONE] val_acc={acc_s2:.4f}\")\n\n\n# =============================================================================\n# SECTION 13 – STAGE 3: FULL FINE-TUNE + MIXUP\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*65)\nprint(\"  STAGE 3 – Full Fine-tune (all layers, Mixup ON)\")\nprint(\"=\"*65)\n\ncore_f = model_f.module if isinstance(model_f, nn.DataParallel) else model_f\ncore_f.ms_enc.unfreeze_all(); core_f.hs_enc.unfreeze_all()\n\nmodel_f, acc_s3 = run_stage(model_f, train_ld, val_ld,\n                              CFG.EPOCHS_S3, CFG.LR / 5, \"fusion\",\n                              \"/kaggle/working/ckpt_fusion_final.pth\",\n                              use_mixup=True)\nprint(f\"[S3 DONE] val_acc={acc_s3:.4f}\")\n\n\n# =============================================================================\n# SECTION 14 – ABLATION SUMMARY\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*65)\nprint(\"  ABLATION SUMMARY\")\nprint(\"=\"*65)\nprint(f\"  S1a  MS  baseline                : {acc_s1a:.4f}\")\nprint(f\"  S1b  HS  core (SE-attn, 40 bands): {acc_s1b:.4f}\")\nprint(f\"  S2   Fusion (partial unfreeze)   : {acc_s2:.4f}\")\nprint(f\"  S3   Fusion (full + Mixup)       : {acc_s3:.4f}\")\nprint(f\"  Previous baseline                : ~0.537\")\nprint(\"=\"*65)\n\n\n# =============================================================================\n# SECTION 15 – INFERENCE & SUBMISSION\n# =============================================================================\n#\n# SUBMISSION FORMAT (exactly as required):\n#   Id        → filename WITH .png extension  e.g. val_02afcb0e.png\n#   Category  → Health | Rust | Other\n#\n# TTA: 4 orientations averaged (original, H-flip, V-flip, 90° rotation)\n# =============================================================================\n\n@torch.no_grad()\ndef run_inference(model, loader, mode):\n    model.eval()\n    rows = []\n    for batch in loader:\n        ms        = batch[\"ms\"].to(device, non_blocking=True)\n        hs        = batch[\"hs\"].to(device, non_blocking=True)\n        filenames = batch[\"filename\"]   # list of \"val_xxxxx.png\" strings\n\n        with autocast('cuda', enabled=USE_AMP):\n            # 4-fold TTA: original + H-flip + V-flip + 90° rotation\n            p0 = F.softmax(_fwd(model, ms,                          hs,                          mode), -1)\n            p1 = F.softmax(_fwd(model, torch.flip(ms, [-1]),        torch.flip(hs, [-1]),        mode), -1)\n            p2 = F.softmax(_fwd(model, torch.flip(ms, [-2]),        torch.flip(hs, [-2]),        mode), -1)\n            p3 = F.softmax(_fwd(model, torch.rot90(ms,1,[-2,-1]),   torch.rot90(hs,1,[-2,-1]),   mode), -1)\n            probs = (p0 + p1 + p2 + p3) / 4.0\n\n        preds = probs.argmax(1).cpu().numpy()\n        for fname, pred in zip(filenames, preds):\n            rows.append({\n                \"Id\":       fname,                        # e.g. val_02afcb0e.png\n                \"Category\": CFG.IDX2CLASS[int(pred)],    # Health | Rust | Other\n            })\n    return rows\n\n\nprint(\"\\n[INFERENCE] Running 4-fold TTA on val set…\")\nrows = run_inference(model_f, test_ld, \"fusion\")\n\n# Build and validate submission dataframe\nsub = pd.DataFrame(rows, columns=[\"Id\", \"Category\"])\nprint(f\"[INFERENCE] Total rows    : {len(sub)}\")\nprint(f\"[INFERENCE] Sample IDs    : {sub['Id'].head(3).tolist()}\")\nprint(f\"[INFERENCE] Class dist    :\\n{sub['Category'].value_counts()}\")\n\n# Safety: ensure all category values are valid\ninvalid = set(sub[\"Category\"].unique()) - set(CFG.CLASSES)\nif invalid:\n    raise ValueError(f\"Invalid class names found in submission: {invalid}\")\n\n# Save to Kaggle working directory\nsub.to_csv(CFG.SUBMISSION, index=False)\n\n# Verify the file was actually written\nsaved_size = Path(CFG.SUBMISSION).stat().st_size\nprint(f\"\\n[SAVED] {CFG.SUBMISSION}\")\nprint(f\"[SAVED] File size: {saved_size} bytes\")\nprint(f\"\\n{sub.head(10).to_string(index=False)}\")\n\nprint(\"\\n\" + \"=\"*65)\nprint(\"  PIPELINE COMPLETE\")\nprint(f\"  MS  val_acc  : {acc_s1a:.4f}\")\nprint(f\"  HS  val_acc  : {acc_s1b:.4f}\")\nprint(f\"  S2  val_acc  : {acc_s2:.4f}\")\nprint(f\"  S3  val_acc  : {acc_s3:.4f}\")\nprint(f\"  Submission   : {CFG.SUBMISSION}\")\nprint(f\"  Format       : Id (filename.png) | Category\")\nprint(\"=\"*65)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T08:38:02.670397Z","iopub.execute_input":"2026-02-20T08:38:02.670867Z","iopub.status.idle":"2026-02-20T09:49:38.655556Z","shell.execute_reply.started":"2026-02-20T08:38:02.670844Z","shell.execute_reply":"2026-02-20T09:49:38.654567Z"}},"outputs":[{"name":"stdout","text":"[HW] device=cuda  GPUs=2  AMP=True\n[HW] GPU=Tesla T4  VRAM=15.6GB\n[PATH] Root: /kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026/Kaggle_Prepared\n[PATH] TRAIN=/kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026/Kaggle_Prepared/train\n[PATH] VAL  =/kaggle/input/beyond-visible-spectrum-ai-for-agriculture-2026/Kaggle_Prepared/val\n[COLLECT] Train: 600 PNG files\n[COLLECT] Train records=600  dist={'Health': 200, 'Other': 200, 'Rust': 200}\n[COLLECT] Val: 300 PNG files\n[COLLECT] Val records=300\n[OK] train=600  val=300\n[INFO] Class weights: [1. 1. 1.]\n\n[DS] int_train=510  int_val=90  test=300\n[DS] train=21 val=4 test=13 batches\n\n=================================================================\n  STAGE 1a – MS Baseline (EfficientNet-B0, 7ch, backbone frozen)\n=================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4256dafbd2a84f46bbbb07a7874d1d21"}},"metadata":{}},{"name":"stdout","text":"\n─────────────────────────────────────────────────────────────────\n  mode=MS  epochs=15  lr=6e-04  mixup=False\n─────────────────────────────────────────────────────────────────\n  ep   1/15  tr 0.5669/0.306  va 0.5372/0.289  lr 1.20e-04\n    Health=0.167 | Rust=0.400 | Other=0.300\n    ✓ checkpoint saved  val_acc=0.2889\n  ep   2/15  tr 0.5033/0.397  va nan/0.367  lr 2.46e-04\n    Health=0.400 | Rust=0.133 | Other=0.567\n    ✓ checkpoint saved  val_acc=0.3667\n  ep   3/15  tr 0.4419/0.498  va 0.5085/0.456  lr 3.72e-04\n    Health=0.467 | Rust=0.133 | Other=0.767\n    ✓ checkpoint saved  val_acc=0.4556\n  ep   4/15  tr 0.4166/0.552  va 0.4422/0.433  lr 4.98e-04\n    Health=0.567 | Rust=0.200 | Other=0.533\n  ep   5/15  tr 0.4201/0.534  va 0.5026/0.433  lr 1.14e-04\n    Health=0.500 | Rust=0.267 | Other=0.533\n  ep   6/15  tr 0.4135/0.514  va 0.4789/0.433  lr 3.07e-05\n    Health=0.400 | Rust=0.367 | Other=0.533\n  ep   7/15  tr 0.4117/0.528  va 0.4875/0.433  lr 6.00e-04\n    Health=0.433 | Rust=0.300 | Other=0.567\n  ep   8/15  tr 0.4220/0.530  va 0.4088/0.533  lr 5.70e-04\n    Health=0.667 | Rust=0.433 | Other=0.500\n    ✓ checkpoint saved  val_acc=0.5333\n  ep   9/15  tr 0.3936/0.558  va 0.4521/0.489  lr 4.87e-04\n    Health=0.467 | Rust=0.467 | Other=0.533\n  ep  10/15  tr 0.4138/0.548  va 0.4625/0.422  lr 3.67e-04\n    Health=0.433 | Rust=0.333 | Other=0.500\n  ep  11/15  tr 0.3853/0.552  va 0.4263/0.478  lr 2.34e-04\n    Health=0.433 | Rust=0.300 | Other=0.700\n  ep  12/15  tr 0.3749/0.567  va 0.4163/0.544  lr 1.14e-04\n    Health=0.433 | Rust=0.467 | Other=0.733\n    ✓ checkpoint saved  val_acc=0.5444\n  ep  13/15  tr 0.3859/0.552  va 0.4654/0.400  lr 3.07e-05\n    Health=0.333 | Rust=0.300 | Other=0.567\n  ep  14/15  tr 0.3901/0.567  va 0.4781/0.400  lr 6.00e-04\n    Health=0.400 | Rust=0.300 | Other=0.500\n  ep  15/15  tr 0.3779/0.599  va 0.4840/0.400  lr 5.70e-04\n    Health=0.333 | Rust=0.333 | Other=0.533\n  Stage best val_acc: 0.5444\n\n  [1a] Unfreezing top-4 backbone blocks…\n\n─────────────────────────────────────────────────────────────────\n  mode=MS  epochs=10  lr=7e-05  mixup=False\n─────────────────────────────────────────────────────────────────\n  ep   1/10  tr 0.3827/0.554  va 0.4393/0.444  lr 2.67e-05\n    Health=0.367 | Rust=0.333 | Other=0.633\n    ✓ checkpoint saved  val_acc=0.4444\n  ep   2/10  tr 0.3652/0.603  va 0.4672/0.422  lr 5.47e-05\n    Health=0.433 | Rust=0.367 | Other=0.467\n  ep   3/10  tr 0.3728/0.573  va 0.4455/0.478  lr 2.37e-05\n    Health=0.467 | Rust=0.400 | Other=0.567\n    ✓ checkpoint saved  val_acc=0.4778\n  ep   4/10  tr 0.3721/0.593  va 0.4590/0.444  lr 7.27e-06\n    Health=0.433 | Rust=0.367 | Other=0.533\n  ep   5/10  tr 0.3402/0.623  va 0.4003/0.544  lr 6.67e-05\n    Health=0.500 | Rust=0.400 | Other=0.733\n    ✓ checkpoint saved  val_acc=0.5444\n  ep   6/10  tr 0.3715/0.607  va 0.4520/0.467  lr 6.04e-05\n    Health=0.400 | Rust=0.400 | Other=0.600\n  ep   7/10  tr 0.3545/0.583  va 0.4107/0.556  lr 4.40e-05\n    Health=0.467 | Rust=0.500 | Other=0.700\n    ✓ checkpoint saved  val_acc=0.5556\n  ep   8/10  tr 0.3567/0.607  va 0.3845/0.567  lr 2.37e-05\n    Health=0.367 | Rust=0.533 | Other=0.800\n    ✓ checkpoint saved  val_acc=0.5667\n  ep   9/10  tr 0.3367/0.623  va 0.4413/0.444  lr 7.27e-06\n    Health=0.400 | Rust=0.333 | Other=0.600\n  ep  10/10  tr 0.3557/0.625  va 0.4448/0.433  lr 6.67e-05\n    Health=0.400 | Rust=0.333 | Other=0.567\n  Stage best val_acc: 0.5667\n[S1a DONE] val_acc=0.5667\n\n=================================================================\n  STAGE 1b – HS Core (EfficientNet-B0, 40ch, SE-attention, frozen)\n=================================================================\n\n─────────────────────────────────────────────────────────────────\n  mode=HS  epochs=15  lr=6e-04  mixup=False\n─────────────────────────────────────────────────────────────────\n  ep   1/15  tr 0.5909/0.335  va 0.6629/0.278  lr 1.20e-04\n    Health=0.133 | Rust=0.000 | Other=0.700\n    ✓ checkpoint saved  val_acc=0.2778\n  ep   2/15  tr 0.4969/0.409  va 0.6746/0.267  lr 2.46e-04\n    Health=0.133 | Rust=0.100 | Other=0.567\n  ep   3/15  tr 0.4624/0.468  va 0.6225/0.367  lr 3.72e-04\n    Health=0.167 | Rust=0.133 | Other=0.800\n    ✓ checkpoint saved  val_acc=0.3667\n  ep   4/15  tr 0.4553/0.484  va 0.4411/0.522  lr 4.98e-04\n    Health=0.433 | Rust=0.500 | Other=0.633\n    ✓ checkpoint saved  val_acc=0.5222\n  ep   5/15  tr 0.4430/0.504  va 0.4608/0.478  lr 1.14e-04\n    Health=0.267 | Rust=0.600 | Other=0.567\n  ep   6/15  tr 0.4277/0.516  va 0.4432/0.500  lr 3.07e-05\n    Health=0.300 | Rust=0.700 | Other=0.500\n  ep   7/15  tr 0.4137/0.534  va 0.4392/0.456  lr 6.00e-04\n    Health=0.267 | Rust=0.533 | Other=0.567\n  ep   8/15  tr 0.4168/0.524  va 0.4312/0.511  lr 5.70e-04\n    Health=0.300 | Rust=0.533 | Other=0.700\n  ep   9/15  tr 0.4271/0.500  va 0.4816/0.489  lr 4.87e-04\n    Health=0.333 | Rust=0.633 | Other=0.500\n  ep  10/15  tr 0.3968/0.552  va 0.4610/0.467  lr 3.67e-04\n    Health=0.300 | Rust=0.600 | Other=0.500\n  ep  11/15  tr 0.4103/0.526  va 0.4763/0.467  lr 2.34e-04\n    Health=0.433 | Rust=0.500 | Other=0.467\n  ep  12/15  tr 0.4057/0.538  va 0.4629/0.478  lr 1.14e-04\n    Health=0.400 | Rust=0.500 | Other=0.533\n  ep  13/15  tr 0.3963/0.562  va 0.4590/0.478  lr 3.07e-05\n    Health=0.433 | Rust=0.433 | Other=0.567\n  ep  14/15  tr 0.3862/0.593  va 0.5099/0.422  lr 6.00e-04\n    Health=0.333 | Rust=0.400 | Other=0.533\n  ep  15/15  tr 0.3992/0.579  va 0.4782/0.456  lr 5.70e-04\n    Health=0.333 | Rust=0.533 | Other=0.500\n  Stage best val_acc: 0.5222\n\n  [1b] Unfreezing top-4 backbone blocks…\n\n─────────────────────────────────────────────────────────────────\n  mode=HS  epochs=10  lr=7e-05  mixup=False\n─────────────────────────────────────────────────────────────────\n  ep   1/10  tr 0.4376/0.496  va 0.4307/0.478  lr 2.67e-05\n    Health=0.367 | Rust=0.400 | Other=0.667\n    ✓ checkpoint saved  val_acc=0.4778\n  ep   2/10  tr 0.4390/0.516  va 0.4484/0.467  lr 5.47e-05\n    Health=0.267 | Rust=0.400 | Other=0.733\n  ep   3/10  tr 0.4282/0.502  va 0.4587/0.478  lr 2.37e-05\n    Health=0.267 | Rust=0.467 | Other=0.700\n  ep   4/10  tr 0.4150/0.558  va 0.4864/0.444  lr 7.27e-06\n    Health=0.367 | Rust=0.467 | Other=0.500\n  ep   5/10  tr 0.4285/0.514  va 0.4751/0.489  lr 6.67e-05\n    Health=0.400 | Rust=0.567 | Other=0.500\n    ✓ checkpoint saved  val_acc=0.4889\n  ep   6/10  tr 0.4071/0.562  va 0.4462/0.489  lr 6.04e-05\n    Health=0.367 | Rust=0.367 | Other=0.733\n  ep   7/10  tr 0.4244/0.540  va 0.4572/0.500  lr 4.40e-05\n    Health=0.433 | Rust=0.600 | Other=0.467\n    ✓ checkpoint saved  val_acc=0.5000\n  ep   8/10  tr 0.3976/0.565  va 0.4696/0.489  lr 2.37e-05\n    Health=0.467 | Rust=0.433 | Other=0.567\n  ep   9/10  tr 0.4129/0.528  va 0.4665/0.400  lr 7.27e-06\n    Health=0.200 | Rust=0.500 | Other=0.500\n  ep  10/10  tr 0.3931/0.544  va 0.4040/0.567  lr 6.67e-05\n    Health=0.400 | Rust=0.600 | Other=0.700\n    ✓ checkpoint saved  val_acc=0.5667\n  Stage best val_acc: 0.5667\n[S1b DONE] val_acc=0.5667\n\n=================================================================\n  STAGE 2 – Gated Fusion (top-4 blocks unfrozen)\n=================================================================\n\n─────────────────────────────────────────────────────────────────\n  mode=FUSION  epochs=20  lr=2e-04  mixup=False\n─────────────────────────────────────────────────────────────────\n  ep   1/20  tr 0.4893/0.321  va 0.4853/0.356  lr 4.00e-05\n    Health=0.033 | Rust=0.167 | Other=0.867\n    ✓ checkpoint saved  val_acc=0.3556\n  ep   2/20  tr 0.4783/0.429  va 0.4764/0.522  lr 8.20e-05\n    Health=0.300 | Rust=0.700 | Other=0.567\n    ✓ checkpoint saved  val_acc=0.5222\n  ep   3/20  tr 0.4607/0.498  va 0.4561/0.556  lr 1.24e-04\n    Health=0.400 | Rust=0.700 | Other=0.567\n    ✓ checkpoint saved  val_acc=0.5556\n  ep   4/20  tr 0.4312/0.546  va 0.4399/0.489  lr 1.66e-04\n    Health=0.200 | Rust=0.667 | Other=0.600\n  ep   5/20  tr 0.3833/0.581  va 0.3976/0.567  lr 1.01e-04\n    Health=0.267 | Rust=0.800 | Other=0.633\n    ✓ checkpoint saved  val_acc=0.5667\n  ep   6/20  tr 0.3684/0.597  va 0.3853/0.533  lr 6.98e-05\n    Health=0.233 | Rust=0.700 | Other=0.667\n  ep   7/20  tr 0.3468/0.587  va 0.3607/0.578  lr 4.20e-05\n    Health=0.433 | Rust=0.600 | Other=0.700\n    ✓ checkpoint saved  val_acc=0.5778\n  ep   8/20  tr 0.3603/0.587  va 0.4246/0.511  lr 2.00e-05\n    Health=0.300 | Rust=0.567 | Other=0.667\n  ep   9/20  tr 0.3437/0.629  va 0.3751/0.544  lr 5.87e-06\n    Health=0.433 | Rust=0.500 | Other=0.700\n  ep  10/20  tr 0.3401/0.647  va 0.3778/0.533  lr 2.00e-04\n    Health=0.433 | Rust=0.533 | Other=0.633\n  ep  11/20  tr 0.3331/0.627  va 0.4383/0.533  lr 1.95e-04\n    Health=0.367 | Rust=0.633 | Other=0.600\n  ep  12/20  tr 0.3342/0.613  va 0.3970/0.556  lr 1.81e-04\n    Health=0.533 | Rust=0.500 | Other=0.633\n  ep  13/20  tr 0.3122/0.659  va 0.3969/0.544  lr 1.59e-04\n    Health=0.500 | Rust=0.467 | Other=0.667\n  ep  14/20  tr 0.3072/0.659  va 0.3616/0.556  lr 1.31e-04\n    Health=0.467 | Rust=0.433 | Other=0.767\n  ep  15/20  tr 0.2827/0.694  va 0.4549/0.456  lr 1.01e-04\n    Health=0.333 | Rust=0.433 | Other=0.600\n  ep  16/20  tr 0.2893/0.698  va 0.3733/0.522  lr 6.98e-05\n    Health=0.400 | Rust=0.433 | Other=0.733\n  ep  17/20  tr 0.2632/0.728  va 0.4417/0.522  lr 4.20e-05\n    Health=0.600 | Rust=0.367 | Other=0.600\n  ep  18/20  tr 0.2545/0.724  va 0.4328/0.556  lr 2.00e-05\n    Health=0.600 | Rust=0.467 | Other=0.600\n  ep  19/20  tr 0.2487/0.734  va 0.4570/0.500  lr 5.87e-06\n    Health=0.433 | Rust=0.433 | Other=0.633\n  ep  20/20  tr 0.2586/0.730  va 0.4785/0.467  lr 2.00e-04\n    Health=0.400 | Rust=0.400 | Other=0.600\n  Stage best val_acc: 0.5778\n[S2 DONE] val_acc=0.5778\n\n=================================================================\n  STAGE 3 – Full Fine-tune (all layers, Mixup ON)\n=================================================================\n\n─────────────────────────────────────────────────────────────────\n  mode=FUSION  epochs=30  lr=4e-05  mixup=True\n─────────────────────────────────────────────────────────────────\n  ep   1/30  tr 0.3919/0.452  va 0.4020/0.489  lr 8.00e-06\n    Health=0.267 | Rust=0.567 | Other=0.633\n    ✓ checkpoint saved  val_acc=0.4889\n  ep   2/30  tr 0.3701/0.494  va 0.4057/0.500  lr 1.64e-05\n    Health=0.267 | Rust=0.533 | Other=0.700\n    ✓ checkpoint saved  val_acc=0.5000\n  ep   3/30  tr 0.3763/0.442  va 0.4089/0.511  lr 2.48e-05\n    Health=0.367 | Rust=0.567 | Other=0.600\n    ✓ checkpoint saved  val_acc=0.5111\n  ep   4/30  tr 0.3657/0.506  va 0.4128/0.489  lr 3.32e-05\n    Health=0.267 | Rust=0.533 | Other=0.667\n  ep   5/30  tr 0.3721/0.482  va 0.4234/0.478  lr 3.03e-05\n    Health=0.267 | Rust=0.533 | Other=0.633\n  ep   6/30  tr 0.3809/0.522  va 0.4142/0.478  lr 2.65e-05\n    Health=0.267 | Rust=0.533 | Other=0.633\n  ep   7/30  tr 0.3856/0.520  va 0.4221/0.489  lr 2.25e-05\n    Health=0.367 | Rust=0.467 | Other=0.633\n  ep   8/30  tr 0.3850/0.433  va 0.4106/0.511  lr 1.85e-05\n    Health=0.467 | Rust=0.467 | Other=0.600\n  ep   9/30  tr 0.3677/0.466  va 0.4025/0.522  lr 1.45e-05\n    Health=0.433 | Rust=0.533 | Other=0.600\n    ✓ checkpoint saved  val_acc=0.5222\n  ep  10/30  tr 0.3620/0.472  va 0.4025/0.522  lr 1.08e-05\n    Health=0.467 | Rust=0.500 | Other=0.600\n  ep  11/30  tr 0.3616/0.470  va 0.4244/0.500  lr 7.45e-06\n    Health=0.567 | Rust=0.333 | Other=0.600\n  ep  12/30  tr 0.3624/0.540  va 0.4061/0.544  lr 4.72e-06\n    Health=0.533 | Rust=0.500 | Other=0.600\n    ✓ checkpoint saved  val_acc=0.5444\n  ep  13/30  tr 0.3748/0.488  va 0.4043/0.467  lr 2.69e-06\n    Health=0.367 | Rust=0.400 | Other=0.633\n  ep  14/30  tr 0.3756/0.458  va 0.4069/0.489  lr 1.43e-06\n    Health=0.433 | Rust=0.400 | Other=0.633\n  ep  15/30  tr 0.3589/0.442  va 0.4276/0.444  lr 4.00e-05\n    Health=0.333 | Rust=0.333 | Other=0.667\n  ep  16/30  tr 0.3607/0.458  va 0.4131/0.500  lr 3.96e-05\n    Health=0.367 | Rust=0.500 | Other=0.633\n  ep  17/30  tr 0.3790/0.421  va 0.4212/0.500  lr 3.83e-05\n    Health=0.433 | Rust=0.433 | Other=0.633\n  ep  18/30  tr 0.3640/0.438  va 0.4020/0.456  lr 3.63e-05\n    Health=0.300 | Rust=0.367 | Other=0.700\n  ep  19/30  tr 0.3699/0.442  va 0.4168/0.489  lr 3.35e-05\n    Health=0.433 | Rust=0.400 | Other=0.633\n  ep  20/30  tr 0.3723/0.472  va 0.4129/0.533  lr 3.03e-05\n    Health=0.533 | Rust=0.467 | Other=0.600\n  ep  21/30  tr 0.3360/0.486  va 0.3709/0.544  lr 2.65e-05\n    Health=0.400 | Rust=0.433 | Other=0.800\n  ep  22/30  tr 0.3685/0.458  va 0.4195/0.456  lr 2.25e-05\n    Health=0.333 | Rust=0.433 | Other=0.600\n  ep  23/30  tr 0.3734/0.532  va 0.3966/0.578  lr 1.85e-05\n    Health=0.567 | Rust=0.600 | Other=0.567\n    ✓ checkpoint saved  val_acc=0.5778\n  ep  24/30  tr 0.3398/0.532  va 0.4164/0.522  lr 1.45e-05\n    Health=0.467 | Rust=0.500 | Other=0.600\n  ep  25/30  tr 0.3499/0.534  va 0.4256/0.522  lr 1.08e-05\n    Health=0.400 | Rust=0.533 | Other=0.633\n  ep  26/30  tr 0.3718/0.506  va 0.4042/0.533  lr 7.45e-06\n    Health=0.600 | Rust=0.333 | Other=0.667\n  ep  27/30  tr 0.3281/0.476  va 0.4010/0.522  lr 4.72e-06\n    Health=0.467 | Rust=0.467 | Other=0.633\n  ep  28/30  tr 0.3557/0.552  va 0.4131/0.533  lr 2.69e-06\n    Health=0.533 | Rust=0.500 | Other=0.567\n  ep  29/30  tr 0.3504/0.458  va 0.4168/0.500  lr 1.43e-06\n    Health=0.400 | Rust=0.433 | Other=0.667\n  ep  30/30  tr 0.3786/0.482  va 0.4213/0.511  lr 4.00e-05\n    Health=0.533 | Rust=0.433 | Other=0.567\n  Stage best val_acc: 0.5778\n[S3 DONE] val_acc=0.5778\n\n=================================================================\n  ABLATION SUMMARY\n=================================================================\n  S1a  MS  baseline                : 0.5667\n  S1b  HS  core (SE-attn, 40 bands): 0.5667\n  S2   Fusion (partial unfreeze)   : 0.5778\n  S3   Fusion (full + Mixup)       : 0.5778\n  Previous baseline                : ~0.537\n=================================================================\n\n[INFERENCE] Running 4-fold TTA on val set…\n[INFERENCE] Total rows    : 300\n[INFERENCE] Sample IDs    : ['val_000a83c1.png', 'val_00a704b1.png', 'val_01dde030.png']\n[INFERENCE] Class dist    :\nCategory\nRust      114\nOther      98\nHealth     88\nName: count, dtype: int64\n\n[SAVED] /kaggle/working/submission.csv\n[SAVED] File size: 6886 bytes\n\n              Id Category\nval_000a83c1.png    Other\nval_00a704b1.png    Other\nval_01dde030.png   Health\nval_024df365.png     Rust\nval_02afcb0e.png   Health\nval_03864ba6.png     Rust\nval_0537e324.png     Rust\nval_059983e0.png   Health\nval_05cee914.png   Health\nval_07af871a.png   Health\n\n=================================================================\n  PIPELINE COMPLETE\n  MS  val_acc  : 0.5667\n  HS  val_acc  : 0.5667\n  S2  val_acc  : 0.5778\n  S3  val_acc  : 0.5778\n  Submission   : /kaggle/working/submission.csv\n  Format       : Id (filename.png) | Category\n=================================================================\n","output_type":"stream"}],"execution_count":1}]}